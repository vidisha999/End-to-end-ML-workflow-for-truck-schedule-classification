{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0e983b",
   "metadata": {},
   "source": [
    "# **Build End-to-End ML Pipeline for Truck Delay Classification Part 2**\n",
    "\n",
    "\n",
    "The project addresses a critical challenge faced by the logistics industry. Delayed truck shipments not only result in increased operational costs but also impact customer satisfaction. Timely delivery of goods is essential to meet customer expectations and maintain the competitiveness of logistics companies.\n",
    "By accurately predicting truck delays, logistics companies can:\n",
    "* Improve operational efficiency by allocating resources more effectively\n",
    "* Enhance customer satisfaction by providing more reliable delivery schedules\n",
    "* Optimize route planning to reduce delays caused by traffic or adverse weather conditions\n",
    "* Reduce costs associated with delayed shipments, such as penalties or compensation to customers\n",
    "\n",
    "In the first phase of our three-part series, [Learn to Build an End-to-End Machine Learning Pipeline - Part 1](https://www.projectpro.io/project-use-case/build-an-end-to-end-machine-learning-pipeline-for-a-classification-model), we laid the groundwork by utilizing PostgreSQL and MySQL in AWS RDS for data storage, setting up an AWS Sagemaker Notebook, performing data retrieval, conducting exploratory data analysis, and creating feature groups with Hopsworks. \n",
    "\n",
    "In Part 2, we delve deeper into the machine-learning pipeline. Focusing on data retrieval from the feature store, train-validation-test split, one-hot encoding, scaling numerical features, and leveraging Weights and Biases for model experimentation, we will build our pipeline for model building with logistic regression, random forest, and XGBoost models. Further, we explore hyperparameter tuning with sweeps, discuss grid and random search, and, ultimately, the deployment of a Streamlit application on AWS. \n",
    "\n",
    "\n",
    "**Note:  AWS Usage Charges**\n",
    "This project leverages the AWS cloud platform to build the end-to-end machine learning pipeline. While using AWS services, it's important to note that certain activities may incur charges. We recommend exploring the AWS Free Tier, which provides limited access to a wide range of AWS services for 12 months. Please refer to the AWS Free Tier page for detailed information, including eligible services and usage limitations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![image.png](https://images.pexels.com/photos/2199293/pexels-photo-2199293.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Approach**\n",
    "\n",
    "* Data Retrieval from Hopsworks:\n",
    "    * Connecting Hopsworks with Python.\n",
    "    * Retrieving data directly from the feature store.\n",
    "\n",
    "\n",
    "* Train-Validation-Test Split\n",
    "\n",
    "* One-Hot Encoding\n",
    "\n",
    "* Scaling Numerical Features\n",
    "\n",
    "* Model Experimentation and Tracking:\n",
    "    * Weights and Biases Introduction\n",
    "    * Setting up a new project and connecting it to Python\n",
    "\n",
    "\n",
    "* Model Building\n",
    "    * Logistic Regression\n",
    "    * Random Forest\n",
    "    * XGBoost\n",
    "\n",
    "\n",
    "* Hyperparameter Tuning with Sweeps\n",
    "\n",
    "\n",
    "* Streamlit Application Development and Fetching the Best Model\n",
    "\n",
    "\n",
    "* Deployment on AWS EC2 Instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **System Requirements**\n",
    "\n",
    "* python version : 3.10.12\n",
    "\n",
    "## **Library Requirements**\n",
    "\n",
    "* hopsworks==3.4.3\n",
    "* streamlit==1.29.0\n",
    "* pandas==1.5.3\n",
    "* joblib==1.3.2\n",
    "* wandb==0.16.1\n",
    "* xgboost==2.0.2\n",
    "* scikit_learn==1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Key Takeaways**\n",
    "\n",
    "\n",
    "\n",
    "* How to connect Python with Hopsworks and fetch data?\n",
    "\n",
    "* Understand the significance of train validation test data splitting\n",
    "* Implement one-hot encoding for categorical variables.\n",
    "* Distinguish between fit-transform and transform, storing for future use.\n",
    "* Implement normalization techniques in Python.\n",
    "* Understand the significance of experiment tracking.\n",
    "* How to connect with Weights and Biases for model experimentation?\n",
    "* Implement and Track Logistic regression, Random forest, and XGBoost models.\n",
    "* Explore model evaluation metrics and their business implications.\n",
    "* Utilize hyperparameter sweeps in Weights and Biases for tuning.\n",
    "* Learn to fetch the best model from Weights and Biases\n",
    "* Develop a Streamlit application and deploy it on AWS EC2 instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7476aef",
   "metadata": {},
   "source": [
    "### **Data Retrieval from Hopsworks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f56408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4663054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/105624\n"
     ]
    }
   ],
   "source": [
    "# Login to hopsworks by entering api key value\n",
    "project = hopsworks.login(api_key_value='<enter_your_api_key>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73066443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Get the feature store\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc74e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve final\n",
    "final_data = fs.get_feature_group('final_data', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7383bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the query\n",
    "query = final_data.select_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6cc4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hive (3.70s) \n"
     ]
    }
   ],
   "source": [
    "# Read all data\n",
    "final_merge = query.read(read_options={\"use_hive\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ef559c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>truck_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>estimated_arrival</th>\n",
       "      <th>delay</th>\n",
       "      <th>route_avg_temp</th>\n",
       "      <th>route_avg_wind_speed</th>\n",
       "      <th>route_avg_precip</th>\n",
       "      <th>route_avg_humidity</th>\n",
       "      <th>...</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>experience</th>\n",
       "      <th>driving_style</th>\n",
       "      <th>ratings</th>\n",
       "      <th>vehicle_no</th>\n",
       "      <th>average_speed_mph</th>\n",
       "      <th>is_midnight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>637</td>\n",
       "      <td>27585963</td>\n",
       "      <td>R-41fb82a2</td>\n",
       "      <td>2019-02-12 07:00:00</td>\n",
       "      <td>2019-02-12 13:59:24</td>\n",
       "      <td>0</td>\n",
       "      <td>54.333333</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5001910e-8</td>\n",
       "      <td>Carlos Torres</td>\n",
       "      <td>male</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>proactive</td>\n",
       "      <td>2</td>\n",
       "      <td>27585963</td>\n",
       "      <td>63.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4088</td>\n",
       "      <td>18855810</td>\n",
       "      <td>R-c061582f</td>\n",
       "      <td>2019-02-06 07:00:00</td>\n",
       "      <td>2019-02-06 19:22:48</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>728ab9fc-f</td>\n",
       "      <td>John Vasquez</td>\n",
       "      <td>male</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>conservative</td>\n",
       "      <td>4</td>\n",
       "      <td>18855810</td>\n",
       "      <td>48.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6758</td>\n",
       "      <td>31562809</td>\n",
       "      <td>R-a61d33ae</td>\n",
       "      <td>2019-01-10 07:00:00</td>\n",
       "      <td>2019-01-10 14:12:00</td>\n",
       "      <td>1</td>\n",
       "      <td>66.333333</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>84.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>49fe8aed-8</td>\n",
       "      <td>Kurt Smith</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>proactive</td>\n",
       "      <td>8</td>\n",
       "      <td>31562809</td>\n",
       "      <td>63.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9783</td>\n",
       "      <td>61984883</td>\n",
       "      <td>R-d87e53cd</td>\n",
       "      <td>2019-02-09 07:00:00</td>\n",
       "      <td>2019-02-09 20:38:24</td>\n",
       "      <td>0</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>42aa7479-5</td>\n",
       "      <td>Jerry Powers</td>\n",
       "      <td>male</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>conservative</td>\n",
       "      <td>2</td>\n",
       "      <td>61984883</td>\n",
       "      <td>56.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8042</td>\n",
       "      <td>17692634</td>\n",
       "      <td>R-ab28b5f1</td>\n",
       "      <td>2019-02-06 07:00:00</td>\n",
       "      <td>2019-02-06 16:25:48</td>\n",
       "      <td>0</td>\n",
       "      <td>67.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>458adb7d-5</td>\n",
       "      <td>John Coleman</td>\n",
       "      <td>male</td>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>conservative</td>\n",
       "      <td>6</td>\n",
       "      <td>17692634</td>\n",
       "      <td>45.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id  truck_id    route_id      departure_date   estimated_arrival  \\\n",
       "0        637  27585963  R-41fb82a2 2019-02-12 07:00:00 2019-02-12 13:59:24   \n",
       "1       4088  18855810  R-c061582f 2019-02-06 07:00:00 2019-02-06 19:22:48   \n",
       "2       6758  31562809  R-a61d33ae 2019-01-10 07:00:00 2019-01-10 14:12:00   \n",
       "3       9783  61984883  R-d87e53cd 2019-02-09 07:00:00 2019-02-09 20:38:24   \n",
       "4       8042  17692634  R-ab28b5f1 2019-02-06 07:00:00 2019-02-06 16:25:48   \n",
       "\n",
       "   delay  route_avg_temp  route_avg_wind_speed  route_avg_precip  \\\n",
       "0      0       54.333333              5.666667          0.000000   \n",
       "1      0       22.000000             10.000000          0.000000   \n",
       "2      1       66.333333              8.666667          0.033333   \n",
       "3      0       80.500000             10.000000          0.000000   \n",
       "4      0       67.333333             10.666667          0.033333   \n",
       "\n",
       "   route_avg_humidity  ...   driver_id           name gender age experience  \\\n",
       "0           54.000000  ...  5001910e-8  Carlos Torres   male  38          9   \n",
       "1           73.500000  ...  728ab9fc-f   John Vasquez   male  47          4   \n",
       "2           84.333333  ...  49fe8aed-8     Kurt Smith   male  42         13   \n",
       "3           63.500000  ...  42aa7479-5   Jerry Powers   male  41          7   \n",
       "4           90.000000  ...  458adb7d-5   John Coleman   male  50         13   \n",
       "\n",
       "  driving_style ratings  vehicle_no  average_speed_mph  is_midnight  \n",
       "0     proactive       2    27585963              63.31            0  \n",
       "1  conservative       4    18855810              48.03            0  \n",
       "2     proactive       8    31562809              63.74            0  \n",
       "3  conservative       2    61984883              56.94            0  \n",
       "4  conservative       6    17692634              45.07            0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First five rows\n",
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8adbecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12308 entries, 0 to 12307\n",
      "Data columns (total 49 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   unique_id                       12308 non-null  int64         \n",
      " 1   truck_id                        12308 non-null  int64         \n",
      " 2   route_id                        12308 non-null  object        \n",
      " 3   departure_date                  12308 non-null  datetime64[ns]\n",
      " 4   estimated_arrival               12308 non-null  datetime64[ns]\n",
      " 5   delay                           12308 non-null  int64         \n",
      " 6   route_avg_temp                  12308 non-null  float64       \n",
      " 7   route_avg_wind_speed            12308 non-null  float64       \n",
      " 8   route_avg_precip                12308 non-null  float64       \n",
      " 9   route_avg_humidity              12308 non-null  float64       \n",
      " 10  route_avg_visibility            12308 non-null  float64       \n",
      " 11  route_avg_pressure              12308 non-null  float64       \n",
      " 12  route_description               12308 non-null  object        \n",
      " 13  estimated_arrival_nearest_hour  12308 non-null  datetime64[ns]\n",
      " 14  departure_date_nearest_hour     12308 non-null  datetime64[ns]\n",
      " 15  origin_id                       12308 non-null  object        \n",
      " 16  destination_id                  12308 non-null  object        \n",
      " 17  distance                        12308 non-null  float64       \n",
      " 18  average_hours                   12308 non-null  float64       \n",
      " 19  origin_temp                     12304 non-null  float64       \n",
      " 20  origin_wind_speed               12304 non-null  float64       \n",
      " 21  origin_description              12308 non-null  object        \n",
      " 22  origin_precip                   12304 non-null  float64       \n",
      " 23  origin_humidity                 12304 non-null  float64       \n",
      " 24  origin_visibility               12304 non-null  float64       \n",
      " 25  origin_pressure                 12304 non-null  float64       \n",
      " 26  destination_temp                12308 non-null  float64       \n",
      " 27  destination_wind_speed          12308 non-null  float64       \n",
      " 28  destination_description         12308 non-null  object        \n",
      " 29  destination_precip              12308 non-null  float64       \n",
      " 30  destination_humidity            12308 non-null  int64         \n",
      " 31  destination_visibility          12308 non-null  float64       \n",
      " 32  destination_pressure            12308 non-null  float64       \n",
      " 33  avg_no_of_vehicles              12308 non-null  float64       \n",
      " 34  accident                        12308 non-null  int64         \n",
      " 35  truck_age                       12308 non-null  int64         \n",
      " 36  load_capacity_pounds            11704 non-null  float64       \n",
      " 37  mileage_mpg                     12308 non-null  int64         \n",
      " 38  fuel_type                       12308 non-null  object        \n",
      " 39  driver_id                       12308 non-null  object        \n",
      " 40  name                            12308 non-null  object        \n",
      " 41  gender                          12308 non-null  object        \n",
      " 42  age                             12308 non-null  int64         \n",
      " 43  experience                      12308 non-null  int64         \n",
      " 44  driving_style                   12308 non-null  object        \n",
      " 45  ratings                         12308 non-null  int64         \n",
      " 46  vehicle_no                      12308 non-null  int64         \n",
      " 47  average_speed_mph               12308 non-null  float64       \n",
      " 48  is_midnight                     12308 non-null  int64         \n",
      "dtypes: datetime64[ns](4), float64(22), int64(12), object(11)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Basic Information on the dataframe\n",
    "final_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f1d49ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_id                           0\n",
       "truck_id                            0\n",
       "route_id                            0\n",
       "departure_date                      0\n",
       "estimated_arrival                   0\n",
       "delay                               0\n",
       "route_avg_temp                      0\n",
       "route_avg_wind_speed                0\n",
       "route_avg_precip                    0\n",
       "route_avg_humidity                  0\n",
       "route_avg_visibility                0\n",
       "route_avg_pressure                  0\n",
       "route_description                   0\n",
       "estimated_arrival_nearest_hour      0\n",
       "departure_date_nearest_hour         0\n",
       "origin_id                           0\n",
       "destination_id                      0\n",
       "distance                            0\n",
       "average_hours                       0\n",
       "origin_temp                         4\n",
       "origin_wind_speed                   4\n",
       "origin_description                  0\n",
       "origin_precip                       4\n",
       "origin_humidity                     4\n",
       "origin_visibility                   4\n",
       "origin_pressure                     4\n",
       "destination_temp                    0\n",
       "destination_wind_speed              0\n",
       "destination_description             0\n",
       "destination_precip                  0\n",
       "destination_humidity                0\n",
       "destination_visibility              0\n",
       "destination_pressure                0\n",
       "avg_no_of_vehicles                  0\n",
       "accident                            0\n",
       "truck_age                           0\n",
       "load_capacity_pounds              604\n",
       "mileage_mpg                         0\n",
       "fuel_type                           0\n",
       "driver_id                           0\n",
       "name                                0\n",
       "gender                              0\n",
       "age                                 0\n",
       "experience                          0\n",
       "driving_style                       0\n",
       "ratings                             0\n",
       "vehicle_no                          0\n",
       "average_speed_mph                   0\n",
       "is_midnight                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of null values\n",
    "final_merge.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36c39ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>truck_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>estimated_arrival</th>\n",
       "      <th>delay</th>\n",
       "      <th>route_avg_temp</th>\n",
       "      <th>route_avg_wind_speed</th>\n",
       "      <th>route_avg_precip</th>\n",
       "      <th>route_avg_humidity</th>\n",
       "      <th>...</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>experience</th>\n",
       "      <th>driving_style</th>\n",
       "      <th>ratings</th>\n",
       "      <th>vehicle_no</th>\n",
       "      <th>average_speed_mph</th>\n",
       "      <th>is_midnight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>7662</td>\n",
       "      <td>18091756</td>\n",
       "      <td>R-112b790b</td>\n",
       "      <td>2019-01-25 07:00:00</td>\n",
       "      <td>2019-01-27 02:40:48</td>\n",
       "      <td>1</td>\n",
       "      <td>66.555556</td>\n",
       "      <td>6.888889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>e975a383-c</td>\n",
       "      <td>Neil Herring</td>\n",
       "      <td>male</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>proactive</td>\n",
       "      <td>3</td>\n",
       "      <td>18091756</td>\n",
       "      <td>58.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9692</th>\n",
       "      <td>11359</td>\n",
       "      <td>22916520</td>\n",
       "      <td>R-78ee1f97</td>\n",
       "      <td>2019-01-25 07:00:00</td>\n",
       "      <td>2019-01-28 10:08:24</td>\n",
       "      <td>0</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>10.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>ffedbf74-a</td>\n",
       "      <td>Thomas Ochoa</td>\n",
       "      <td>male</td>\n",
       "      <td>57</td>\n",
       "      <td>19</td>\n",
       "      <td>proactive</td>\n",
       "      <td>6</td>\n",
       "      <td>22916520</td>\n",
       "      <td>63.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9768</th>\n",
       "      <td>8165</td>\n",
       "      <td>24746768</td>\n",
       "      <td>R-b5f9418a</td>\n",
       "      <td>2019-01-25 07:00:00</td>\n",
       "      <td>2019-01-27 14:35:24</td>\n",
       "      <td>0</td>\n",
       "      <td>47.454545</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.636364</td>\n",
       "      <td>...</td>\n",
       "      <td>3d91387f-2</td>\n",
       "      <td>William Anderson III</td>\n",
       "      <td>male</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>conservative</td>\n",
       "      <td>4</td>\n",
       "      <td>24746768</td>\n",
       "      <td>40.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12063</th>\n",
       "      <td>7721</td>\n",
       "      <td>24654257</td>\n",
       "      <td>R-21472caf</td>\n",
       "      <td>2019-01-25 07:00:00</td>\n",
       "      <td>2019-01-27 16:50:24</td>\n",
       "      <td>0</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>12.363636</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>79.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>f110642c-1</td>\n",
       "      <td>Marc Walters</td>\n",
       "      <td>male</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>proactive</td>\n",
       "      <td>3</td>\n",
       "      <td>24654257</td>\n",
       "      <td>61.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id  truck_id    route_id      departure_date  \\\n",
       "3086        7662  18091756  R-112b790b 2019-01-25 07:00:00   \n",
       "9692       11359  22916520  R-78ee1f97 2019-01-25 07:00:00   \n",
       "9768        8165  24746768  R-b5f9418a 2019-01-25 07:00:00   \n",
       "12063       7721  24654257  R-21472caf 2019-01-25 07:00:00   \n",
       "\n",
       "        estimated_arrival  delay  route_avg_temp  route_avg_wind_speed  \\\n",
       "3086  2019-01-27 02:40:48      1       66.555556              6.888889   \n",
       "9692  2019-01-28 10:08:24      0       57.500000             10.142857   \n",
       "9768  2019-01-27 14:35:24      0       47.454545              9.090909   \n",
       "12063 2019-01-27 16:50:24      0       69.000000             12.363636   \n",
       "\n",
       "       route_avg_precip  route_avg_humidity  ...   driver_id  \\\n",
       "3086           0.000000           90.888889  ...  e975a383-c   \n",
       "9692           0.000000           78.214286  ...  ffedbf74-a   \n",
       "9768           0.000000           70.636364  ...  3d91387f-2   \n",
       "12063          0.018182           79.181818  ...  f110642c-1   \n",
       "\n",
       "                       name gender age experience driving_style ratings  \\\n",
       "3086           Neil Herring   male  45          7     proactive       3   \n",
       "9692           Thomas Ochoa   male  57         19     proactive       6   \n",
       "9768   William Anderson III   male  50          0  conservative       4   \n",
       "12063          Marc Walters   male  47          5     proactive       3   \n",
       "\n",
       "       vehicle_no  average_speed_mph  is_midnight  \n",
       "3086     18091756              58.02            1  \n",
       "9692     22916520              63.64            1  \n",
       "9768     24746768              40.69            1  \n",
       "12063    24654257              61.93            1  \n",
       "\n",
       "[4 rows x 49 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the rows where origin temp is null\n",
    "final_merge[final_merge['origin_temp'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d519cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>truck_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>estimated_arrival</th>\n",
       "      <th>delay</th>\n",
       "      <th>route_avg_temp</th>\n",
       "      <th>route_avg_wind_speed</th>\n",
       "      <th>route_avg_precip</th>\n",
       "      <th>route_avg_humidity</th>\n",
       "      <th>...</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>experience</th>\n",
       "      <th>driving_style</th>\n",
       "      <th>ratings</th>\n",
       "      <th>vehicle_no</th>\n",
       "      <th>average_speed_mph</th>\n",
       "      <th>is_midnight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>7662</td>\n",
       "      <td>18091756</td>\n",
       "      <td>R-112b790b</td>\n",
       "      <td>2019-01-25 07:00:00</td>\n",
       "      <td>2019-01-27 02:40:48</td>\n",
       "      <td>1</td>\n",
       "      <td>66.555556</td>\n",
       "      <td>6.888889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.888889</td>\n",
       "      <td>...</td>\n",
       "      <td>e975a383-c</td>\n",
       "      <td>Neil Herring</td>\n",
       "      <td>male</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>proactive</td>\n",
       "      <td>3</td>\n",
       "      <td>18091756</td>\n",
       "      <td>58.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9692</th>\n",
       "      <td>11359</td>\n",
       "      <td>22916520</td>\n",
       "      <td>R-78ee1f97</td>\n",
       "      <td>2019-01-25 07:00:00</td>\n",
       "      <td>2019-01-28 10:08:24</td>\n",
       "      <td>0</td>\n",
       "      <td>57.500000</td>\n",
       "      <td>10.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.214286</td>\n",
       "      <td>...</td>\n",
       "      <td>ffedbf74-a</td>\n",
       "      <td>Thomas Ochoa</td>\n",
       "      <td>male</td>\n",
       "      <td>57</td>\n",
       "      <td>19</td>\n",
       "      <td>proactive</td>\n",
       "      <td>6</td>\n",
       "      <td>22916520</td>\n",
       "      <td>63.64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9768</th>\n",
       "      <td>8165</td>\n",
       "      <td>24746768</td>\n",
       "      <td>R-b5f9418a</td>\n",
       "      <td>2019-01-25 07:00:00</td>\n",
       "      <td>2019-01-27 14:35:24</td>\n",
       "      <td>0</td>\n",
       "      <td>47.454545</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.636364</td>\n",
       "      <td>...</td>\n",
       "      <td>3d91387f-2</td>\n",
       "      <td>William Anderson III</td>\n",
       "      <td>male</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>conservative</td>\n",
       "      <td>4</td>\n",
       "      <td>24746768</td>\n",
       "      <td>40.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12063</th>\n",
       "      <td>7721</td>\n",
       "      <td>24654257</td>\n",
       "      <td>R-21472caf</td>\n",
       "      <td>2019-01-25 07:00:00</td>\n",
       "      <td>2019-01-27 16:50:24</td>\n",
       "      <td>0</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>12.363636</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>79.181818</td>\n",
       "      <td>...</td>\n",
       "      <td>f110642c-1</td>\n",
       "      <td>Marc Walters</td>\n",
       "      <td>male</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>proactive</td>\n",
       "      <td>3</td>\n",
       "      <td>24654257</td>\n",
       "      <td>61.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id  truck_id    route_id      departure_date  \\\n",
       "3086        7662  18091756  R-112b790b 2019-01-25 07:00:00   \n",
       "9692       11359  22916520  R-78ee1f97 2019-01-25 07:00:00   \n",
       "9768        8165  24746768  R-b5f9418a 2019-01-25 07:00:00   \n",
       "12063       7721  24654257  R-21472caf 2019-01-25 07:00:00   \n",
       "\n",
       "        estimated_arrival  delay  route_avg_temp  route_avg_wind_speed  \\\n",
       "3086  2019-01-27 02:40:48      1       66.555556              6.888889   \n",
       "9692  2019-01-28 10:08:24      0       57.500000             10.142857   \n",
       "9768  2019-01-27 14:35:24      0       47.454545              9.090909   \n",
       "12063 2019-01-27 16:50:24      0       69.000000             12.363636   \n",
       "\n",
       "       route_avg_precip  route_avg_humidity  ...   driver_id  \\\n",
       "3086           0.000000           90.888889  ...  e975a383-c   \n",
       "9692           0.000000           78.214286  ...  ffedbf74-a   \n",
       "9768           0.000000           70.636364  ...  3d91387f-2   \n",
       "12063          0.018182           79.181818  ...  f110642c-1   \n",
       "\n",
       "                       name gender age experience driving_style ratings  \\\n",
       "3086           Neil Herring   male  45          7     proactive       3   \n",
       "9692           Thomas Ochoa   male  57         19     proactive       6   \n",
       "9768   William Anderson III   male  50          0  conservative       4   \n",
       "12063          Marc Walters   male  47          5     proactive       3   \n",
       "\n",
       "       vehicle_no  average_speed_mph  is_midnight  \n",
       "3086     18091756              58.02            1  \n",
       "9692     22916520              63.64            1  \n",
       "9768     24746768              40.69            1  \n",
       "12063    24654257              61.93            1  \n",
       "\n",
       "[4 rows x 49 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the rows where origin humidity is null\n",
    "# Looks like we have null values in the same rows, let's find out which origin city is this\n",
    "final_merge[final_merge['origin_humidity'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a32f7ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hive (1.20s) \n"
     ]
    }
   ],
   "source": [
    "# Fetch the routes data\n",
    "routes_data = fs.get_feature_group('routes_details_fg', version=1)\n",
    "\n",
    "routes_data_query = routes_data.select_all()\n",
    "\n",
    "routes_df = routes_data_query.read(read_options={\"use_hive\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b25ff4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>origin_id</th>\n",
       "      <th>destination_id</th>\n",
       "      <th>distance</th>\n",
       "      <th>average_hours</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R-b5f9418a</td>\n",
       "      <td>C-f8f01604</td>\n",
       "      <td>C-4fe0fa24</td>\n",
       "      <td>2779.33</td>\n",
       "      <td>55.59</td>\n",
       "      <td>2023-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>R-21472caf</td>\n",
       "      <td>C-f8f01604</td>\n",
       "      <td>C-2e349ccd</td>\n",
       "      <td>2892.14</td>\n",
       "      <td>57.84</td>\n",
       "      <td>2023-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>R-112b790b</td>\n",
       "      <td>C-f8f01604</td>\n",
       "      <td>C-d3bb431c</td>\n",
       "      <td>2183.94</td>\n",
       "      <td>43.68</td>\n",
       "      <td>2023-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>R-78ee1f97</td>\n",
       "      <td>C-f8f01604</td>\n",
       "      <td>C-f5ed4c15</td>\n",
       "      <td>3757.02</td>\n",
       "      <td>75.14</td>\n",
       "      <td>2023-08-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        route_id   origin_id destination_id  distance  average_hours  \\\n",
       "6     R-b5f9418a  C-f8f01604     C-4fe0fa24   2779.33          55.59   \n",
       "438   R-21472caf  C-f8f01604     C-2e349ccd   2892.14          57.84   \n",
       "702   R-112b790b  C-f8f01604     C-d3bb431c   2183.94          43.68   \n",
       "1290  R-78ee1f97  C-f8f01604     C-f5ed4c15   3757.02          75.14   \n",
       "\n",
       "     event_time  \n",
       "6    2023-08-23  \n",
       "438  2023-08-23  \n",
       "702  2023-08-23  \n",
       "1290 2023-08-23  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the rows with the routes ids which has no info on origin city's weather on 25th jan\n",
    "# Only 1 city is there in all these rows\n",
    "routes_df[routes_df.route_id.isin(['R-112b790b', 'R-78ee1f97','R-b5f9418a', 'R-21472caf'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bb1254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: ssl.PROTOCOL_TLS is deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hive (3.05s) \n"
     ]
    }
   ],
   "source": [
    "# Let's check if we have any information on this city\n",
    "# Fetching the weather data\n",
    "weather_data = fs.get_feature_group('city_weather_details_fg', version=1)\n",
    "\n",
    "weather_query = weather_data.select_all()\n",
    "\n",
    "weather_df = weather_query.read(read_options={\"use_hive\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ffd1b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_id</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>temp</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>description</th>\n",
       "      <th>precip</th>\n",
       "      <th>humidity</th>\n",
       "      <th>visibility</th>\n",
       "      <th>pressure</th>\n",
       "      <th>chanceofrain</th>\n",
       "      <th>chanceoffog</th>\n",
       "      <th>chanceofsnow</th>\n",
       "      <th>chanceofthunder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [city_id, date, hour, temp, wind_speed, description, precip, humidity, visibility, pressure, chanceofrain, chanceoffog, chanceofsnow, chanceofthunder]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the weather data with city and date\n",
    "# We don't have any information on this, we will remove these rows\n",
    "# It is important to check with the business regarding the information though \n",
    "weather_df[(weather_df.city_id=='C-f8f01604')&(weather_df.date==pd.to_datetime('2019-01-25'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b85a239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows\n",
    "\n",
    "final_merge=final_merge.dropna(subset =  ['origin_temp', 'origin_wind_speed', 'origin_precip',\n",
    "                                'origin_humidity', 'origin_visibility', 'origin_pressure' ] ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04248a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_id                           0\n",
       "truck_id                            0\n",
       "route_id                            0\n",
       "departure_date                      0\n",
       "estimated_arrival                   0\n",
       "delay                               0\n",
       "route_avg_temp                      0\n",
       "route_avg_wind_speed                0\n",
       "route_avg_precip                    0\n",
       "route_avg_humidity                  0\n",
       "route_avg_visibility                0\n",
       "route_avg_pressure                  0\n",
       "route_description                   0\n",
       "estimated_arrival_nearest_hour      0\n",
       "departure_date_nearest_hour         0\n",
       "origin_id                           0\n",
       "destination_id                      0\n",
       "distance                            0\n",
       "average_hours                       0\n",
       "origin_temp                         0\n",
       "origin_wind_speed                   0\n",
       "origin_description                  0\n",
       "origin_precip                       0\n",
       "origin_humidity                     0\n",
       "origin_visibility                   0\n",
       "origin_pressure                     0\n",
       "destination_temp                    0\n",
       "destination_wind_speed              0\n",
       "destination_description             0\n",
       "destination_precip                  0\n",
       "destination_humidity                0\n",
       "destination_visibility              0\n",
       "destination_pressure                0\n",
       "avg_no_of_vehicles                  0\n",
       "accident                            0\n",
       "truck_age                           0\n",
       "load_capacity_pounds              604\n",
       "mileage_mpg                         0\n",
       "fuel_type                           0\n",
       "driver_id                           0\n",
       "name                                0\n",
       "gender                              0\n",
       "age                                 0\n",
       "experience                          0\n",
       "driving_style                       0\n",
       "ratings                             0\n",
       "vehicle_no                          0\n",
       "average_speed_mph                   0\n",
       "is_midnight                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's verify the dropped null values\n",
    "final_merge.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22e5abed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>truck_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>departure_date</th>\n",
       "      <th>estimated_arrival</th>\n",
       "      <th>delay</th>\n",
       "      <th>route_avg_temp</th>\n",
       "      <th>route_avg_wind_speed</th>\n",
       "      <th>route_avg_precip</th>\n",
       "      <th>route_avg_humidity</th>\n",
       "      <th>...</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>experience</th>\n",
       "      <th>driving_style</th>\n",
       "      <th>ratings</th>\n",
       "      <th>vehicle_no</th>\n",
       "      <th>average_speed_mph</th>\n",
       "      <th>is_midnight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>637</td>\n",
       "      <td>27585963</td>\n",
       "      <td>R-41fb82a2</td>\n",
       "      <td>2019-02-12 07:00:00</td>\n",
       "      <td>2019-02-12 13:59:24</td>\n",
       "      <td>0</td>\n",
       "      <td>54.333333</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5001910e-8</td>\n",
       "      <td>Carlos Torres</td>\n",
       "      <td>male</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>proactive</td>\n",
       "      <td>2</td>\n",
       "      <td>27585963</td>\n",
       "      <td>63.31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4088</td>\n",
       "      <td>18855810</td>\n",
       "      <td>R-c061582f</td>\n",
       "      <td>2019-02-06 07:00:00</td>\n",
       "      <td>2019-02-06 19:22:48</td>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>728ab9fc-f</td>\n",
       "      <td>John Vasquez</td>\n",
       "      <td>male</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>conservative</td>\n",
       "      <td>4</td>\n",
       "      <td>18855810</td>\n",
       "      <td>48.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6758</td>\n",
       "      <td>31562809</td>\n",
       "      <td>R-a61d33ae</td>\n",
       "      <td>2019-01-10 07:00:00</td>\n",
       "      <td>2019-01-10 14:12:00</td>\n",
       "      <td>1</td>\n",
       "      <td>66.333333</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>84.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>49fe8aed-8</td>\n",
       "      <td>Kurt Smith</td>\n",
       "      <td>male</td>\n",
       "      <td>42</td>\n",
       "      <td>13</td>\n",
       "      <td>proactive</td>\n",
       "      <td>8</td>\n",
       "      <td>31562809</td>\n",
       "      <td>63.74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9783</td>\n",
       "      <td>61984883</td>\n",
       "      <td>R-d87e53cd</td>\n",
       "      <td>2019-02-09 07:00:00</td>\n",
       "      <td>2019-02-09 20:38:24</td>\n",
       "      <td>0</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>42aa7479-5</td>\n",
       "      <td>Jerry Powers</td>\n",
       "      <td>male</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>conservative</td>\n",
       "      <td>2</td>\n",
       "      <td>61984883</td>\n",
       "      <td>56.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8042</td>\n",
       "      <td>17692634</td>\n",
       "      <td>R-ab28b5f1</td>\n",
       "      <td>2019-02-06 07:00:00</td>\n",
       "      <td>2019-02-06 16:25:48</td>\n",
       "      <td>0</td>\n",
       "      <td>67.333333</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>458adb7d-5</td>\n",
       "      <td>John Coleman</td>\n",
       "      <td>male</td>\n",
       "      <td>50</td>\n",
       "      <td>13</td>\n",
       "      <td>conservative</td>\n",
       "      <td>6</td>\n",
       "      <td>17692634</td>\n",
       "      <td>45.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12299</th>\n",
       "      <td>1846</td>\n",
       "      <td>66683794</td>\n",
       "      <td>R-a51bd34b</td>\n",
       "      <td>2019-01-06 07:00:00</td>\n",
       "      <td>2019-01-07 09:28:48</td>\n",
       "      <td>0</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>138dd3a4-5</td>\n",
       "      <td>Bradley Ramirez</td>\n",
       "      <td>male</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>proactive</td>\n",
       "      <td>4</td>\n",
       "      <td>66683794</td>\n",
       "      <td>63.14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12300</th>\n",
       "      <td>10298</td>\n",
       "      <td>20252337</td>\n",
       "      <td>R-2b5aba88</td>\n",
       "      <td>2019-02-10 07:00:00</td>\n",
       "      <td>2019-02-11 13:02:24</td>\n",
       "      <td>1</td>\n",
       "      <td>58.571429</td>\n",
       "      <td>10.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.428571</td>\n",
       "      <td>...</td>\n",
       "      <td>3bb7faad-d</td>\n",
       "      <td>Renee Cuevas</td>\n",
       "      <td>female</td>\n",
       "      <td>43</td>\n",
       "      <td>12</td>\n",
       "      <td>proactive</td>\n",
       "      <td>4</td>\n",
       "      <td>20252337</td>\n",
       "      <td>62.82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12301</th>\n",
       "      <td>9490</td>\n",
       "      <td>21949967</td>\n",
       "      <td>R-cf631f68</td>\n",
       "      <td>2019-01-25 07:00:00</td>\n",
       "      <td>2019-01-25 22:06:00</td>\n",
       "      <td>0</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>9.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>2363c29b-1</td>\n",
       "      <td>Zachary Hardy</td>\n",
       "      <td>male</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>conservative</td>\n",
       "      <td>8</td>\n",
       "      <td>21949967</td>\n",
       "      <td>41.56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12302</th>\n",
       "      <td>1716</td>\n",
       "      <td>96362807</td>\n",
       "      <td>R-83e7feed</td>\n",
       "      <td>2019-01-07 07:00:00</td>\n",
       "      <td>2019-01-07 21:33:00</td>\n",
       "      <td>1</td>\n",
       "      <td>71.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1ed64257-3</td>\n",
       "      <td>Steven Walton</td>\n",
       "      <td>male</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>proactive</td>\n",
       "      <td>3</td>\n",
       "      <td>96362807</td>\n",
       "      <td>54.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12303</th>\n",
       "      <td>10139</td>\n",
       "      <td>69327588</td>\n",
       "      <td>R-53c30281</td>\n",
       "      <td>2019-02-02 07:00:00</td>\n",
       "      <td>2019-02-04 11:44:24</td>\n",
       "      <td>0</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>74aad901-0</td>\n",
       "      <td>Keith Jones DDS</td>\n",
       "      <td>male</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>conservative</td>\n",
       "      <td>3</td>\n",
       "      <td>69327588</td>\n",
       "      <td>41.34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12304 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id  truck_id    route_id      departure_date  \\\n",
       "0            637  27585963  R-41fb82a2 2019-02-12 07:00:00   \n",
       "1           4088  18855810  R-c061582f 2019-02-06 07:00:00   \n",
       "2           6758  31562809  R-a61d33ae 2019-01-10 07:00:00   \n",
       "3           9783  61984883  R-d87e53cd 2019-02-09 07:00:00   \n",
       "4           8042  17692634  R-ab28b5f1 2019-02-06 07:00:00   \n",
       "...          ...       ...         ...                 ...   \n",
       "12299       1846  66683794  R-a51bd34b 2019-01-06 07:00:00   \n",
       "12300      10298  20252337  R-2b5aba88 2019-02-10 07:00:00   \n",
       "12301       9490  21949967  R-cf631f68 2019-01-25 07:00:00   \n",
       "12302       1716  96362807  R-83e7feed 2019-01-07 07:00:00   \n",
       "12303      10139  69327588  R-53c30281 2019-02-02 07:00:00   \n",
       "\n",
       "        estimated_arrival  delay  route_avg_temp  route_avg_wind_speed  \\\n",
       "0     2019-02-12 13:59:24      0       54.333333              5.666667   \n",
       "1     2019-02-06 19:22:48      0       22.000000             10.000000   \n",
       "2     2019-01-10 14:12:00      1       66.333333              8.666667   \n",
       "3     2019-02-09 20:38:24      0       80.500000             10.000000   \n",
       "4     2019-02-06 16:25:48      0       67.333333             10.666667   \n",
       "...                   ...    ...             ...                   ...   \n",
       "12299 2019-01-07 09:28:48      0       59.000000              9.333333   \n",
       "12300 2019-02-11 13:02:24      1       58.571429             10.428571   \n",
       "12301 2019-01-25 22:06:00      0       45.250000              9.750000   \n",
       "12302 2019-01-07 21:33:00      1       71.500000              7.000000   \n",
       "12303 2019-02-04 11:44:24      0       34.000000              9.000000   \n",
       "\n",
       "       route_avg_precip  route_avg_humidity  ...   driver_id             name  \\\n",
       "0              0.000000           54.000000  ...  5001910e-8    Carlos Torres   \n",
       "1              0.000000           73.500000  ...  728ab9fc-f     John Vasquez   \n",
       "2              0.033333           84.333333  ...  49fe8aed-8       Kurt Smith   \n",
       "3              0.000000           63.500000  ...  42aa7479-5     Jerry Powers   \n",
       "4              0.033333           90.000000  ...  458adb7d-5     John Coleman   \n",
       "...                 ...                 ...  ...         ...              ...   \n",
       "12299          0.000000           89.333333  ...  138dd3a4-5  Bradley Ramirez   \n",
       "12300          0.000000           57.428571  ...  3bb7faad-d     Renee Cuevas   \n",
       "12301          0.000000           71.250000  ...  2363c29b-1    Zachary Hardy   \n",
       "12302          0.000000           56.500000  ...  1ed64257-3    Steven Walton   \n",
       "12303          0.000000           94.500000  ...  74aad901-0  Keith Jones DDS   \n",
       "\n",
       "       gender age experience driving_style ratings  vehicle_no  \\\n",
       "0        male  38          9     proactive       2    27585963   \n",
       "1        male  47          4  conservative       4    18855810   \n",
       "2        male  42         13     proactive       8    31562809   \n",
       "3        male  41          7  conservative       2    61984883   \n",
       "4        male  50         13  conservative       6    17692634   \n",
       "...       ...  ..        ...           ...     ...         ...   \n",
       "12299    male  47          9     proactive       4    66683794   \n",
       "12300  female  43         12     proactive       4    20252337   \n",
       "12301    male  41          4  conservative       8    21949967   \n",
       "12302    male  43         10     proactive       3    96362807   \n",
       "12303    male  45          7  conservative       3    69327588   \n",
       "\n",
       "       average_speed_mph  is_midnight  \n",
       "0                  63.31            0  \n",
       "1                  48.03            0  \n",
       "2                  63.74            0  \n",
       "3                  56.94            0  \n",
       "4                  45.07            0  \n",
       "...                  ...          ...  \n",
       "12299              63.14            1  \n",
       "12300              62.82            1  \n",
       "12301              41.56            0  \n",
       "12302              54.95            0  \n",
       "12303              41.34            1  \n",
       "\n",
       "[12304 rows x 49 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train - Validation - Test Split**\n",
    "\n",
    "The data points are divided into two or three datasets, train and test, in a train test split method and train validation test split in three way split. The train data is used to train the model, and the model is then used to predict on the test data to see how the model performs on unseen data and whether it is overfitting or underfitting.\n",
    "\n",
    "\n",
    "The validation set is a different set of data from the training set that is used to validate the performance of our model during training. This validation approach gives data that allows us to fine-tune the model's hyperparameters and configurations.\n",
    " \n",
    "Once model optimization is done with the help of the validation set, the model is then used to test unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3785392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting necessary columns and removing id columns\n",
    "\n",
    "cts_cols=['route_avg_temp', 'route_avg_wind_speed',\n",
    "       'route_avg_precip', 'route_avg_humidity', 'route_avg_visibility',\n",
    "       'route_avg_pressure', 'distance', 'average_hours',\n",
    "       'origin_temp', 'origin_wind_speed', 'origin_precip', 'origin_humidity',\n",
    "       'origin_visibility', 'origin_pressure',\n",
    "       'destination_temp','destination_wind_speed','destination_precip',\n",
    "       'destination_humidity', 'destination_visibility','destination_pressure',\n",
    "        'avg_no_of_vehicles', 'truck_age','load_capacity_pounds', 'mileage_mpg',\n",
    "        'age', 'experience','average_speed_mph']\n",
    "\n",
    "\n",
    "cat_cols=['route_description',\n",
    "       'origin_description', 'destination_description',\n",
    "        'accident', 'fuel_type',\n",
    "       'gender', 'driving_style', 'ratings','is_midnight']\n",
    "\n",
    "\n",
    "target=['delay']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "669a556b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2019-01-01 07:04:48'), Timestamp('2019-02-14 16:06:00'))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the date range\n",
    "final_merge['estimated_arrival'].min(), final_merge['estimated_arrival'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d49e9192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training, validation, and test sets based on date\n",
    "\n",
    "train_df = final_merge[final_merge['estimated_arrival'] <= pd.to_datetime('2019-01-30')]\n",
    "\n",
    "validation_df = final_merge[(final_merge['estimated_arrival'] > pd.to_datetime('2019-01-30')) &\n",
    "\n",
    "                            (final_merge['estimated_arrival'] <= pd.to_datetime('2019-02-07'))]\n",
    "\n",
    "test_df = final_merge[final_merge['estimated_arrival'] > pd.to_datetime('2019-02-07')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c73d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_df[cts_cols+cat_cols]\n",
    "\n",
    "y_train=train_df['delay']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "827b2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = validation_df[cts_cols + cat_cols]\n",
    "\n",
    "y_valid = validation_df['delay']\n",
    "\n",
    "X_test=test_df[cts_cols+cat_cols]\n",
    "\n",
    "y_test=test_df['delay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preprocessing and Leakage**\n",
    "\n",
    "Data leakage is a situation where information from the test or prediction data is inadvertently used during the training process of a machine learning model. This can occur when information from the test or prediction data is leaked into the training data, and the model uses this information to improve its performance during the training process.\n",
    "\n",
    "Data leakage can occur during the preprocessing phase of machine learning when information from the test or prediction data is used to preprocess the training data, inadvertently leaking information from the test or prediction data into the training data.\n",
    "\n",
    "For example, consider a scenario where the preprocessing step involves imputing missing values in the dataset. If the missing values are imputed using the mean or median values of the entire dataset, including the test and prediction data, then the imputed values in the training data may be influenced by the values in the test and prediction data. This can lead to data leakage, as the model may learn to recognize patterns in the test and prediction data during the training process, leading to overfitting and poor generalization performance.\n",
    "\n",
    "\n",
    "To avoid data leakage, it's important to perform the data preprocessing steps on the training data only, and then apply the same preprocessing steps to the test and prediction data separately. This ensures that the test and prediction data remain unseen by the model during the training process, and helps to prevent overfitting and improve the accuracy of the model.\n",
    "\n",
    "In the context of this problem, we performed all data preprocessing steps together for the sake of simplicity, which could potentially lead to data leakage. However, in real-world scenarios, it's important to treat the test and prediction data separately and apply the necessary preprocessing steps separately, based on the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e50146a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3000.0\n",
       "Name: load_capacity_pounds, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_capacity_mode = X_train['load_capacity_pounds'].mode()\n",
    "\n",
    "load_capacity_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ec68202",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['load_capacity_pounds']=X_train['load_capacity_pounds'].fillna(load_capacity_mode.iloc[0])\n",
    "X_valid['load_capacity_pounds']=X_valid['load_capacity_pounds'].fillna(load_capacity_mode.iloc[0])\n",
    "X_test['load_capacity_pounds']=X_test['load_capacity_pounds'].fillna(load_capacity_mode.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40707ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "route_avg_temp             0\n",
       "route_avg_wind_speed       0\n",
       "route_avg_precip           0\n",
       "route_avg_humidity         0\n",
       "route_avg_visibility       0\n",
       "route_avg_pressure         0\n",
       "distance                   0\n",
       "average_hours              0\n",
       "origin_temp                0\n",
       "origin_wind_speed          0\n",
       "origin_precip              0\n",
       "origin_humidity            0\n",
       "origin_visibility          0\n",
       "origin_pressure            0\n",
       "destination_temp           0\n",
       "destination_wind_speed     0\n",
       "destination_precip         0\n",
       "destination_humidity       0\n",
       "destination_visibility     0\n",
       "destination_pressure       0\n",
       "avg_no_of_vehicles         0\n",
       "truck_age                  0\n",
       "load_capacity_pounds       0\n",
       "mileage_mpg                0\n",
       "age                        0\n",
       "experience                 0\n",
       "average_speed_mph          0\n",
       "route_description          0\n",
       "origin_description         0\n",
       "destination_description    0\n",
       "accident                   0\n",
       "fuel_type                  0\n",
       "gender                     0\n",
       "driving_style              0\n",
       "ratings                    0\n",
       "is_midnight                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f3d5233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "route_avg_temp             0\n",
       "route_avg_wind_speed       0\n",
       "route_avg_precip           0\n",
       "route_avg_humidity         0\n",
       "route_avg_visibility       0\n",
       "route_avg_pressure         0\n",
       "distance                   0\n",
       "average_hours              0\n",
       "origin_temp                0\n",
       "origin_wind_speed          0\n",
       "origin_precip              0\n",
       "origin_humidity            0\n",
       "origin_visibility          0\n",
       "origin_pressure            0\n",
       "destination_temp           0\n",
       "destination_wind_speed     0\n",
       "destination_precip         0\n",
       "destination_humidity       0\n",
       "destination_visibility     0\n",
       "destination_pressure       0\n",
       "avg_no_of_vehicles         0\n",
       "truck_age                  0\n",
       "load_capacity_pounds       0\n",
       "mileage_mpg                0\n",
       "age                        0\n",
       "experience                 0\n",
       "average_speed_mph          0\n",
       "route_description          0\n",
       "origin_description         0\n",
       "destination_description    0\n",
       "accident                   0\n",
       "fuel_type                  0\n",
       "gender                     0\n",
       "driving_style              0\n",
       "ratings                    0\n",
       "is_midnight                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b902550d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "route_avg_temp             0\n",
       "route_avg_wind_speed       0\n",
       "route_avg_precip           0\n",
       "route_avg_humidity         0\n",
       "route_avg_visibility       0\n",
       "route_avg_pressure         0\n",
       "distance                   0\n",
       "average_hours              0\n",
       "origin_temp                0\n",
       "origin_wind_speed          0\n",
       "origin_precip              0\n",
       "origin_humidity            0\n",
       "origin_visibility          0\n",
       "origin_pressure            0\n",
       "destination_temp           0\n",
       "destination_wind_speed     0\n",
       "destination_precip         0\n",
       "destination_humidity       0\n",
       "destination_visibility     0\n",
       "destination_pressure       0\n",
       "avg_no_of_vehicles         0\n",
       "truck_age                  0\n",
       "load_capacity_pounds       0\n",
       "mileage_mpg                0\n",
       "age                        0\n",
       "experience                 0\n",
       "average_speed_mph          0\n",
       "route_description          0\n",
       "origin_description         0\n",
       "destination_description    0\n",
       "accident                   0\n",
       "fuel_type                  0\n",
       "gender                     0\n",
       "driving_style              0\n",
       "ratings                    0\n",
       "is_midnight                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6ee395",
   "metadata": {},
   "source": [
    "### **Encoding**\n",
    "\n",
    "\n",
    "Encoding is a process in machine learning that involves converting categorical data, which consists of non-numeric labels, into a numerical format. This transformation is necessary because many machine learning algorithms operate on numerical data, and categorical variables need to be represented in a way that the algorithms can understand and process effectively.\n",
    "\n",
    "**Why Encoding is Needed:**\n",
    "\n",
    "Most machine learning algorithms require numerical input. Categorical data, which includes labels like 'red' or 'blue,' must be encoded into numeric values for these algorithms to work properly.\n",
    "\n",
    "Categorical variables often have no inherent order or numerical meaning. Encoding allows us to represent them numerically without implying any ordinal relationships.\n",
    "\n",
    "Types of Encoding:\n",
    "\n",
    "* One-Hot Encoding: It creates binary columns for each category and indicates the presence of the category with a 1 and absence with a 0. If we have colors as categories ('Red', 'Blue', 'Green'), one-hot encoding would create three binary columns, each representing one color.\n",
    "\n",
    "\n",
    "* Label Encoding: It assigns a unique numerical label to each category. The labels are usually integers. They are mostly used when the categories have inherent order or ranking.\n",
    "Example: If we have sizes ('Small', 'Medium', 'Large'), label encoding might assign 1 to 'Small,' 2 to 'Medium,' and 3 to 'Large.'\n",
    "\n",
    "\n",
    "* Target Encoding (Mean Encoding): It involves replacing a categorical value with the mean of the target variable for that category.\n",
    " If we have a binary target variable (0 or 1) and a categorical feature like 'Country,' target encoding would replace each country with the mean of the target variable for that country.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae668372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Standard Scaler and One-Hot Encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pickle import dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f279a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the One-Hot Encoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cd62023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying columns to be encoded\n",
    "encode_columns = ['route_description', 'origin_description', 'destination_description', 'fuel_type', 'gender', 'driving_style']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ef9777f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse=False, sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse=False, sparse_output=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore', sparse=False, sparse_output=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the encoder on the training data\n",
    "encoder.fit(X_train[encode_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ae42a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating names for the new one-hot encoded features\n",
    "encoded_features = list(encoder.get_feature_names_out(encode_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feaef126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['route_description_Blizzard',\n",
       " 'route_description_Blowing snow',\n",
       " 'route_description_Clear',\n",
       " 'route_description_Cloudy',\n",
       " 'route_description_Fog',\n",
       " 'route_description_Freezing drizzle',\n",
       " 'route_description_Freezing fog',\n",
       " 'route_description_Heavy rain',\n",
       " 'route_description_Heavy rain at times',\n",
       " 'route_description_Heavy snow',\n",
       " 'route_description_Light drizzle',\n",
       " 'route_description_Light freezing rain',\n",
       " 'route_description_Light rain',\n",
       " 'route_description_Light rain shower',\n",
       " 'route_description_Light sleet',\n",
       " 'route_description_Light sleet showers',\n",
       " 'route_description_Light snow',\n",
       " 'route_description_Mist',\n",
       " 'route_description_Moderate or heavy freezing rain',\n",
       " 'route_description_Moderate or heavy rain shower',\n",
       " 'route_description_Moderate or heavy rain with thunder',\n",
       " 'route_description_Moderate or heavy sleet',\n",
       " 'route_description_Moderate or heavy sleet showers',\n",
       " 'route_description_Moderate or heavy snow showers',\n",
       " 'route_description_Moderate or heavy snow with thunder',\n",
       " 'route_description_Moderate rain',\n",
       " 'route_description_Moderate rain at times',\n",
       " 'route_description_Moderate snow',\n",
       " 'route_description_Overcast',\n",
       " 'route_description_Partly cloudy',\n",
       " 'route_description_Patchy heavy snow',\n",
       " 'route_description_Patchy light drizzle',\n",
       " 'route_description_Patchy light rain',\n",
       " 'route_description_Patchy light rain with thunder',\n",
       " 'route_description_Patchy light snow',\n",
       " 'route_description_Patchy light snow with thunder',\n",
       " 'route_description_Patchy moderate snow',\n",
       " 'route_description_Patchy rain possible',\n",
       " 'route_description_Patchy sleet possible',\n",
       " 'route_description_Patchy snow possible',\n",
       " 'route_description_Sunny',\n",
       " 'route_description_Thundery outbreaks possible',\n",
       " 'route_description_Torrential rain shower',\n",
       " 'origin_description_Blizzard',\n",
       " 'origin_description_Blowing snow',\n",
       " 'origin_description_Cloudy',\n",
       " 'origin_description_Heavy freezing drizzle',\n",
       " 'origin_description_Heavy rain',\n",
       " 'origin_description_Heavy snow',\n",
       " 'origin_description_Ice pellets',\n",
       " 'origin_description_Light drizzle',\n",
       " 'origin_description_Light freezing rain',\n",
       " 'origin_description_Light rain',\n",
       " 'origin_description_Light rain shower',\n",
       " 'origin_description_Light snow',\n",
       " 'origin_description_Mist',\n",
       " 'origin_description_Moderate or heavy freezing rain',\n",
       " 'origin_description_Moderate or heavy rain shower',\n",
       " 'origin_description_Moderate or heavy rain with thunder',\n",
       " 'origin_description_Moderate rain',\n",
       " 'origin_description_Moderate rain at times',\n",
       " 'origin_description_Moderate snow',\n",
       " 'origin_description_Overcast',\n",
       " 'origin_description_Partly cloudy',\n",
       " 'origin_description_Patchy heavy snow',\n",
       " 'origin_description_Patchy light drizzle',\n",
       " 'origin_description_Patchy light rain',\n",
       " 'origin_description_Patchy light snow',\n",
       " 'origin_description_Patchy moderate snow',\n",
       " 'origin_description_Patchy rain possible',\n",
       " 'origin_description_Patchy snow possible',\n",
       " 'origin_description_Sunny',\n",
       " 'origin_description_Torrential rain shower',\n",
       " 'destination_description_Blizzard',\n",
       " 'destination_description_Blowing snow',\n",
       " 'destination_description_Clear',\n",
       " 'destination_description_Cloudy',\n",
       " 'destination_description_Fog',\n",
       " 'destination_description_Freezing fog',\n",
       " 'destination_description_Heavy freezing drizzle',\n",
       " 'destination_description_Heavy rain',\n",
       " 'destination_description_Heavy rain at times',\n",
       " 'destination_description_Heavy snow',\n",
       " 'destination_description_Ice pellets',\n",
       " 'destination_description_Light drizzle',\n",
       " 'destination_description_Light freezing rain',\n",
       " 'destination_description_Light rain',\n",
       " 'destination_description_Light rain shower',\n",
       " 'destination_description_Light sleet',\n",
       " 'destination_description_Light snow',\n",
       " 'destination_description_Light snow showers',\n",
       " 'destination_description_Mist',\n",
       " 'destination_description_Moderate or heavy freezing rain',\n",
       " 'destination_description_Moderate or heavy rain shower',\n",
       " 'destination_description_Moderate or heavy rain with thunder',\n",
       " 'destination_description_Moderate or heavy showers of ice pellets',\n",
       " 'destination_description_Moderate or heavy sleet',\n",
       " 'destination_description_Moderate or heavy snow showers',\n",
       " 'destination_description_Moderate rain',\n",
       " 'destination_description_Moderate rain at times',\n",
       " 'destination_description_Moderate snow',\n",
       " 'destination_description_Overcast',\n",
       " 'destination_description_Partly cloudy',\n",
       " 'destination_description_Patchy heavy snow',\n",
       " 'destination_description_Patchy light drizzle',\n",
       " 'destination_description_Patchy light rain',\n",
       " 'destination_description_Patchy light snow',\n",
       " 'destination_description_Patchy moderate snow',\n",
       " 'destination_description_Patchy rain possible',\n",
       " 'destination_description_Patchy sleet possible',\n",
       " 'destination_description_Patchy snow possible',\n",
       " 'destination_description_Sunny',\n",
       " 'destination_description_Torrential rain shower',\n",
       " 'fuel_type_Unknown',\n",
       " 'fuel_type_diesel',\n",
       " 'fuel_type_gas',\n",
       " 'gender_Unknown',\n",
       " 'gender_female',\n",
       " 'gender_male',\n",
       " 'driving_style_Unknown',\n",
       " 'driving_style_conservative',\n",
       " 'driving_style_proactive']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b1fc72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the training, validation, and test sets\n",
    "\n",
    "X_train[encoded_features] = encoder.transform(X_train[encode_columns])\n",
    "\n",
    "X_valid[encoded_features] = encoder.transform(X_valid[encode_columns])\n",
    "\n",
    "X_test[encoded_features] = encoder.transform(X_test[encode_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "affd1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the encoder for future use\n",
    "dump(encoder, open('truck_data_encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ee96d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the original categorical features\n",
    "\n",
    "X_train = X_train.drop(encode_columns, axis=1)\n",
    "\n",
    "X_valid = X_valid.drop(encode_columns, axis=1)\n",
    "\n",
    "X_test = X_test.drop(encode_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b107af30",
   "metadata": {},
   "source": [
    "### **Scaling Numerical Features**\n",
    "\n",
    "Feature scaling is a crucial preprocessing step in machine learning that involves transforming the range of features (variables) in a dataset to a common scale. This is necessary because many machine learning algorithms are sensitive to the scale of the input features. Feature scaling ensures that all features contribute equally to the model training process and prevents some features from dominating others solely due to their scale.\n",
    "\n",
    "Types of Feature Scaling:\n",
    "\n",
    "* **Min-Max Scaling (Normalization):**\n",
    "\n",
    "    Formula: $$ X_{\\text{normalized}} = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)} $$\n",
    "\n",
    "​   Min-Max scaling transforms each feature to a range between 0 and 1. It subtracts the minimum value of the feature from each data point and then divides by the range (the difference between the maximum and minimum values). This ensures that the transformed data is in the desired range. Suitable for algorithms that rely on distances between data points, such as k-nearest neighbors and support vector machines.\n",
    "Sensitive to outliers.\n",
    "\n",
    "\n",
    "* **Standardization (Z-score):**\n",
    "\n",
    "    Formula: $$X_{\\text{standardized}} = \\frac{X - \\text{mean}(X)}{\\text{std}(X)}$$\n",
    "\n",
    "    Standardization transforms the data to have a mean of 0 and a standard deviation of 1. It subtracts the mean of the feature from each data point and then divides by the standard deviation. This ensures that the transformed data follows a standard normal distribution. Suitable for algorithms that assume a normal distribution of the input features, such as linear regression and neural networks.\n",
    "    Less sensitive to outliers compared to Min-Max scaling.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e49b7da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Scaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e72b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17f8606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Separate Columns\n",
    "\n",
    "# train\n",
    "\n",
    "X_train[cts_cols] = scaler.fit_transform(X_train[cts_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10e3ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid\n",
    "\n",
    "X_valid[cts_cols] = scaler.transform(X_valid[cts_cols])\n",
    "\n",
    "\n",
    "# test\n",
    "\n",
    "X_test[cts_cols] = scaler.transform(X_test[cts_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d2cdd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the scaler to use in transforming test data\n",
    "\n",
    "dump(scaler, open('truck_data_scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41ce16",
   "metadata": {},
   "source": [
    "## **Model Building and Experimentation**\n",
    "\n",
    "Experiment tracking involves systematically recording and managing details related to each model experiment, including hyperparameters, metrics, and data versions.\n",
    "Benefits: Ensures reproducibility, supports comparison of different models, facilitates collaboration, and aids in informed decision-making.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A Model Registry is a centralized system for managing, versioning, and tracking machine learning models throughout their lifecycle.\n",
    "Benefits: Enables version control, streamlines collaboration, provides deployment features, maintains metadata, and enhances reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92714763",
   "metadata": {},
   "source": [
    "### **Connecting to WANDB**\n",
    "\n",
    "Weights and Biases (W&B) is a collaborative platform designed to help machine learning practitioners track, visualize, and analyze their machine learning experiments. It provides tools for experiment management, visualization of model performance, and collaboration among team members. Here's an overview of key aspects:\n",
    "\n",
    "* Experiment Tracking:\n",
    "    W&B allows users to log various parameters, metrics, and artifacts associated with their machine learning experiments. This includes hyperparameters, model architecture details, training and evaluation metrics, and even visualizations.\n",
    "\n",
    "* Visualization:\n",
    "    Users can leverage W&B to create visualizations of their experiment results. This includes charts, graphs, and plots that make it easy to understand how different parameters impact model performance over time.\n",
    "\n",
    "* Hyperparameter Sweeps:\n",
    "    W&B facilitates hyperparameter tuning through sweeps. Users can define a range of hyperparameter values, and W&B will automatically run multiple experiments with different combinations, helping to find the optimal set of hyperparameters for a given task.\n",
    "\n",
    "* Collaboration and Reproducibility:\n",
    "    Teams can use W&B to collaborate on machine learning projects. Experiment results are easily shareable and reproducible, ensuring that team members can understand and replicate each other's work.\n",
    "\n",
    "* Model Registry:\n",
    "    W&B includes a model registry that enables users to version and organize their trained models. This ensures that models can be tracked, compared, and deployed consistently.\n",
    "\n",
    "* Integrations:\n",
    "    W&B integrates seamlessly with various machine learning frameworks and libraries, including TensorFlow, PyTorch, Scikit-learn, and more. This makes it versatile and adaptable to different workflows.\n",
    "\n",
    "\n",
    "For more information, refer to documentation: https://docs.wandb.ai/guides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b1dbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import wandb\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "890d0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login <enter_your_api_key>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0f93417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "552a4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for interacting with W&B\n",
    "\n",
    "USER_NAME = \"enter_your_username\"\n",
    "\n",
    "PROJECT_NAME = \"enter_your_project_name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification Evaluation Metrics**\n",
    "\n",
    "Classification evaluation metrics are used to evaluate the performance of a machine learning model that is trained for classification tasks. Some of the commonly used classification evaluation metrics are F1 score, recall score, confusion matrix, and ROC AUC score. Here's an overview of each of these metrics:\n",
    "\n",
    "**F1 score**: The F1 score is a metric that combines the precision and recall of a model into a single value. It is calculated as the harmonic mean of precision and recall, and is expressed as a value between 0 and 1, where 1 indicates perfect precision and recall.\n",
    "F1 score is the harmonic mean of precision and recall. It is calculated as follows:\n",
    "$$ F1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} $$\n",
    "where precision is the number of true positives divided by the sum of true positives and false positives, and recall is the number of true positives divided by the sum of true positives and false negatives.\n",
    "\n",
    "**Recall**: Use the recall score when the cost of false negatives (i.e., missing instances of a class) is high. For example, in a medical diagnosis problem, the cost of missing a positive case may be high, so recall would be a more appropriate metric.\n",
    "Recall score (also known as sensitivity) is the number of true positives divided by the sum of true positives and false negatives. It is given by the following formula:\n",
    "$$ Recall = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "**Precision**: Precision is another important classification evaluation metric, which is defined as the ratio of true positives to the total predicted positives. It measures the accuracy of positive predictions made by the classifier, i.e., the proportion of positive identifications that were actually correct.\n",
    "The formula for precision is:\n",
    "$$ precision = \\frac{true\\ positive}{true\\ positive + false\\ positive} $$\n",
    "where true positive refers to the cases where the model correctly predicted the positive class, and false positive refers to the cases where the model incorrectly predicted the positive class.\n",
    "Precision is useful when the cost of false positives is high, such as in medical diagnosis or fraud detection, where a false positive can have serious consequences. In such cases, a higher precision indicates that the model is better at identifying true positives and minimizing false positives.\n",
    "\n",
    "**Confusion Matrix**:\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model. It compares the predicted labels with the true labels and counts the number of true positives, false positives, true negatives, and false negatives. Here is an example of a confusion matrix:\n",
    "\n",
    "|          | Actual Positive | Actual Negative |\n",
    "|----------|----------------|----------------|\n",
    "| Predicted Positive | True Positive (TP) | False Positive (FP) |\n",
    "| Predicted Negative | False Negative (FN) | True Negative (TN) |\n",
    "\n",
    "​\n",
    "\n",
    "\n",
    "\n",
    "**ROC AUC Score**:\n",
    "ROC AUC (Receiver Operating Characteristic Area Under the Curve) score is a measure of how well a classifier is able to distinguish between positive and negative classes. It is calculated as the area under the ROC curve. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. TPR is the number of true positives divided by the sum of true positives and false negatives, and FPR is the number of false positives divided by the sum of false positives and true negatives.\n",
    "$$ ROC\\ AUC\\ Score = \\int_0^1 TPR(FPR^{-1}(t)) dt $$\n",
    "where $FPR^{-1}$ is the inverse of the FPR function.\n",
    "\n",
    "**When to use which**:\n",
    "\n",
    "The choice of evaluation metric depends on the specific requirements of the business problem. Here are some general guidelines:\n",
    "\n",
    "* F1 score: Use the F1 score when the class distribution is imbalanced, and when both precision and recall are equally important.\n",
    "\n",
    "* Recall score: Use the recall score when the cost of false negatives (i.e., missing instances of a class) is high. For example, in a medical diagnosis problem, the cost of missing a positive case may be high, so recall would be a more appropriate metric.\n",
    "\n",
    "* Precision: Precision is useful when the cost of false positives is high, such as in medical diagnosis or fraud detection, where a false positive can have serious consequences. In such cases, a higher precision indicates that the model is better at identifying true positives and minimizing false positives.\n",
    "\n",
    "* Confusion matrix: The confusion matrix is a versatile tool that can be used to visualize the performance of a model across different classes. It can be useful for identifying specific areas of the model that need improvement.\n",
    "\n",
    "* ROC AUC score: Use the ROC AUC score when the ability to distinguish between positive and negative classes is important. For example, in a credit scoring problem, the ability to distinguish between good and bad credit risks is crucial.\n",
    "\n",
    "Importance with respect to the business problem:\n",
    "\n",
    "The importance of each evaluation metric varies depending on the business problem. For example, in a spam detection problem, precision may be more important than recall, since false positives (i.e., classifying a non-spam email as spam) may annoy users, while false negatives (i.e., missing a spam email) may not be as harmful. On the other hand, in a disease diagnosis problem, recall may be more important than precision, since missing a positive case (i.e., a false negative) could have serious consequences. Therefore, it is important to choose the evaluation metric that is most relevant to the specific business problem at hand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f902bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing training libraries and evaluation metrics\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2101b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "# #Columns needed to compare metrics\n",
    "comparison_columns = ['Model_Name', 'Train_F1score', 'Train_Recall', 'Valid_F1score', 'Valid_Recall', 'Test_F1score', 'Test_Recall']\n",
    "\n",
    "comparison_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_models(model_name, model_defined_var, X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "  ''' This function predicts and evaluates various models for classification'''\n",
    "\n",
    "  # train predictions\n",
    "  y_train_pred = model_defined_var.predict(X_train)\n",
    "  # train performance\n",
    "  train_f1_score = f1_score(y_train, y_train_pred)\n",
    "  train_recall = recall_score(y_train, y_train_pred)\n",
    "\n",
    "  # validation predictions\n",
    "  y_valid_pred = model_defined_var.predict(X_valid)\n",
    "  # validation performance\n",
    "  valid_f1_score = f1_score(y_valid, y_valid_pred)\n",
    "  valid_recall = recall_score(y_valid, y_valid_pred)\n",
    "\n",
    "  # test predictions\n",
    "  y_pred = model_defined_var.predict(X_test)\n",
    "  # test performance\n",
    "  test_f1_score = f1_score(y_test, y_pred)\n",
    "  test_recall = recall_score(y_test, y_pred)\n",
    "\n",
    "  # Printing performance\n",
    "  print(\"Train Results\")\n",
    "  print(f'F1 Score: {train_f1_score}')\n",
    "  print(f'Recall Score: {train_recall}')\n",
    "  print(f'Confusion Matrix: \\n{confusion_matrix(y_train, y_train_pred)}')\n",
    "  print(f'Area Under Curve: {roc_auc_score(y_train, y_train_pred)}')\n",
    "\n",
    "  print(\" \")\n",
    "\n",
    "  print(\"Validation Results\")\n",
    "  print(f'F1 Score: {valid_f1_score}')\n",
    "  print(f'Recall Score: {valid_recall}')\n",
    "  print(f'Confusion Matrix: \\n{confusion_matrix(y_valid, y_valid_pred)}')\n",
    "  print(f'Area Under Curve: {roc_auc_score(y_valid, y_valid_pred)}')\n",
    "\n",
    "  print(\" \")\n",
    "\n",
    "  print(\"Test Results\")\n",
    "  print(f'F1 Score: {test_f1_score}')\n",
    "  print(f'Recall Score: {test_recall}')\n",
    "  print(f'Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}')\n",
    "  print(f'Area Under Curve: {roc_auc_score(y_test, y_pred)}')\n",
    "\n",
    "  # Saving our results\n",
    "  global comparison_columns\n",
    "  metric_scores = [model_name, train_f1_score, train_recall, valid_f1_score, valid_recall, test_f1_score, test_recall]\n",
    "  final_dict = dict(zip(comparison_columns, metric_scores))\n",
    "  return final_dict\n",
    "\n",
    "\n",
    "final_list = []\n",
    "def add_dic_to_final_df(final_dict):\n",
    "  global final_list\n",
    "  final_list.append(final_dict)\n",
    "  global comparison_df\n",
    "  comparison_df = pd.DataFrame(final_list, columns=comparison_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7596a4",
   "metadata": {},
   "source": [
    "### **Logistic Regression**\n",
    "\n",
    "\n",
    "\n",
    "Logistic regression is a type of machine learning algorithm used for classification problems where we need to predict if something belongs to one category or another. For example, we can use it to predict if a customer will churn or not.\n",
    "\n",
    "The algorithm works by analyzing the relationship between the input variables (such as customer demographics and usage patterns) and the binary output variable (such as churn or no churn). It then estimates the probability of the output variable using a logistic function, which outputs a value between 0 and 1.\n",
    "\n",
    "Logistic regression is actually a type of classification algorithm, but it is called \"logistic regression\" because it uses a logistic function to model the probability of the binary output variable.\n",
    "\n",
    "The term \"regression\" comes from the fact that the logistic regression model is based on a linear combination of the input variables and their associated weights, which is similar to linear regression. However, in linear regression, we predict a continuous output variable, while in logistic regression, we predict a probability of belonging to a particular class.\n",
    "\n",
    "In other words, logistic regression is a regression algorithm that is used for classification problems. The logistic function transforms the output of the regression equation into a probability value between 0 and 1, which can then be used to classify the input variable into one of two categories.\n",
    "\n",
    "Let's see how!!\n",
    "\n",
    "The logistic regression model is based on the logistic function, which maps any real-valued input to a value between 0 and 1. The logistic function is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "sigmoid(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "where $z$ is a linear combination of the input variables and their associated weights. In other words, we calculate $z$ as follows:\n",
    "\n",
    "$$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3$$\n",
    "\n",
    "where $\\beta_0$ is the intercept term and $\\beta_1$, $\\beta_2$, and $\\beta_3$ are the weights associated with the input variables $x_1$, $x_2$, and $x_3$, respectively.\n",
    "\n",
    "The logistic regression model then predicts the probability of the binary outcome (in our example, whether a customer will churn or not) as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "P(y=1|x) = sigmoid(z)\n",
    "\\end{equation}\n",
    "\n",
    "where $y$ is the binary outcome, $x$ is the input variable vector, and $sigmoid(z)$ is the logistic function.\n",
    "\n",
    "To train the logistic regression model, we use a dataset of labeled examples. Each example includes a set of input variables and the corresponding binary outcome. The model is trained by minimizing the cross-entropy loss function, which is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "L(y,\\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\n",
    "\\end{equation}\n",
    "\n",
    "where $y$ is the binary outcome, $\\hat{y}$ is the predicted probability, $N$ is the number of examples, and $\\log$ is the natural logarithm.\n",
    "\n",
    "To minimize the cross-entropy loss function, we use an optimization algorithm such as gradient descent. The gradient of the loss function with respect to each weight is computed using the chain rule of differentiation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial w_j} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}i - y_i) x{ij}\n",
    "\\end{equation}\n",
    "\n",
    "where $x_{ij}$ is the $j$th input variable of the $i$th example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example**\n",
    "Suppose we have the following dataset with three input variables (customer age, monthly bill amount, and number of customer service calls) and a binary output variable (1 for churn and 0 for no churn):\n",
    "\n",
    "\n",
    "We can use logistic regression to build a model that predicts the probability of churn based on these input variables. The logistic function that we use is:\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "where $y$ is the output variable (churn), $x$ is the input variable (age, monthly bill amount, and number of customer service calls), and $z$ is the linear combination of the input variables and their associated weights:\n",
    "\n",
    "$$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3$$\n",
    "\n",
    "where $\\beta_0$ is the intercept term and $\\beta_1$, $\\beta_2$, and $\\beta_3$ are the weights associated with the input variables $x_1$, $x_2$, and $x_3$, respectively.\n",
    "\n",
    "To train the model, we start with some initial values for the weights and use a training algorithm to adjust the weights iteratively until we minimize the error between the predicted probability and the actual output. The training algorithm typically uses a gradient descent approach to update the weights in the direction that minimizes the loss function.\n",
    "\n",
    "Once the model is trained, we can use it to predict the probability of churn for a new customer. For example, suppose we want to predict the probability of churn for a customer who is 40 years old, has a monthly bill amount of 180, and has made 2 customer service calls. Using the logistic function and the weights learned during training, we can calculate the probability as:\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3)}}$$\n",
    "\n",
    "Let's say the weights learned during training are $\\beta_0 = -2$, $\\beta_1 = 0.05$, $\\beta_2 = 0.01$, and $\\beta_3 = 0.8$. Then we can plug in the values for the new customer and get:\n",
    "\n",
    "$$P(y=1|x) = \\frac{1}{1+e^{-(-2 + 0.05\\times 40 + 0.01\\times 180 + 0.8\\times 2)}} \\approx 0.69$$\n",
    "\n",
    "So, the model predicts that there is a 69% probability that this customer will churn. We can use this probability to classify the customer as churn or no churn, depending on a threshold that we set (e.g., if the probability is above 0.5, we classify the customer as churn).\n",
    "\n",
    "This is a simple example, but it illustrates how logistic regression uses the logistic function and linear combination of input variables to predict the probability of a binary output variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cfc364",
   "metadata": {},
   "source": [
    "## **Class weights**\n",
    "\n",
    "To handle class imbalances, logistic regression allows you to assign weights to the classes. These weights influence the model during training, and they are used to give more importance to the minority class. The goal is to ensure that the model doesn't overly favor the majority class and can still make accurate predictions for the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5355c1f",
   "metadata": {},
   "source": [
    "$$w_j = \\frac{\\text{n\\_samples}}{\\text{n\\_classes} \\times \\text{n\\_samples}_j}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f4e85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.value_counts().to_dict()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "345f4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = len(X_train)/(2*(y_train.value_counts().to_dict()[0])), len(X_train)/(2*(y_train.value_counts().to_dict()[1]))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a202ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "log_reg = LogisticRegression(random_state=13, class_weight={0:weights[0], 1:weights[1]})\n",
    "# fit it\n",
    "log_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119513d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_results = evaluate_models(\"Logistic Regression\", log_reg, X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "add_dic_to_final_df(logistic_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175be690",
   "metadata": {},
   "source": [
    "#### **Logistic Regression - Experiment Tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da33efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "w = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "def train_logistic_model(X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test, y_test=y_test):\n",
    "    features = X_train.columns\n",
    "\n",
    "    with wandb.init(project=PROJECT_NAME) as run:\n",
    "        config = wandb.config\n",
    "        params= {\"random_state\":13,\n",
    "    \"class_weight\":w}\n",
    "\n",
    "        model = LogisticRegression(**params)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # train predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        # train performance\n",
    "        train_f1_score = f1_score(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "        # validation predictions\n",
    "        y_valid_pred = model.predict(X_valid)\n",
    "        # validation performance\n",
    "        valid_f1_score = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "        \n",
    "        # test predictions\n",
    "        y_preds = model.predict(X_test)\n",
    "        y_probas = model.predict_proba(X_test)\n",
    "\n",
    "        score = f1_score(y_test, y_preds)\n",
    "        print(f\"F1_score Train: {round(train_f1_score, 4)}\")\n",
    "        print(f\"F1_score Valid: {round(valid_f1_score, 4)}\")\n",
    "        print(f\"F1_score Test: {round(score, 4)}\")\n",
    "\n",
    "\n",
    "        wandb.log({\"f1_score_train\": train_f1_score})\n",
    "        wandb.log({\"f1_score_valid\": valid_f1_score})\n",
    "        wandb.log({\"f1_score\": score})\n",
    "\n",
    "        wandb.sklearn.plot_classifier(model, X_train, X_test, y_train, y_test,\n",
    "                                            y_preds, y_probas, labels= None, model_name='LogisticRegression', feature_names=features)\n",
    "\n",
    "        model_artifact = wandb.Artifact(\n",
    "                    \"LogisticRegression\", type=\"model\",metadata=dict(config))\n",
    "\n",
    "        joblib.dump(model, \"log-truck-model.pkl\")\n",
    "        model_artifact.add_file(\"log-truck-model.pkl\")\n",
    "        wandb.save(\"log-truck-model.pkl\")\n",
    "        run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762962fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logistic_model(X_train, y_train,X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decision Trees**\n",
    "\n",
    "**Decision Trees in Classification**\n",
    "\n",
    "Decision trees are a type of supervised learning algorithm that can be used for classification as well as regression problems. They are widely used in machine learning because they are easy to understand and interpret, and can handle both categorical and numerical data. The idea behind decision trees is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "The decision tree starts with a single node, called the root node, which represents the entire dataset. The root node is then split into several child nodes based on the value of a chosen feature. The process of selecting the best feature and splitting the nodes is repeated recursively for each child node until a stopping criterion is reached. This results in a tree-like structure that represents the decision rules learned from the data.\n",
    "\n",
    "Each node in the decision tree represents a decision or a test of a feature value, and each branch represents the possible outcomes of that decision. The leaves of the tree represent the final decision or the class label assigned to the input data.\n",
    "\n",
    "**Splitting Criteria**\n",
    "\n",
    "To build a decision tree, we need a measure that determines how to split the data at each node. The splitting criterion is chosen based on the type of data and the nature of the problem. The most common splitting criteria are:\n",
    "\n",
    "* Gini index: measures the impurity of a set of labels. It calculates the probability of misclassifying a randomly chosen element from the set, and is used to minimize misclassification errors.\n",
    "* Information gain: measures the reduction in entropy (uncertainty) after a split. It is used to maximize the information gain in each split.\n",
    "* Chi-square: measures the difference between observed and expected frequencies of the classes. It is used to minimize the deviation between the observed and expected class distribution.\n",
    "\n",
    "**Overfitting in Decision Trees**\n",
    "\n",
    "One of the main challenges in building decision trees is overfitting. Overfitting occurs when the tree is too complex and fits the training data too well, resulting in poor performance on new and unseen data. This can be addressed by pruning the tree or limiting its depth, or by using ensemble methods such as bagging and boosting.\n",
    "\n",
    "**Ensemble Methods**\n",
    "\n",
    "Ensemble methods are techniques that combine multiple models to improve performance and reduce overfitting. The two most common ensemble methods used with decision trees are:\n",
    "\n",
    "* Bagging (Bootstrap Aggregating): involves training multiple decision trees on different subsets of the training data and then combining their predictions by averaging or voting. This reduces the variance and improves the stability of the model.\n",
    "* Boosting: involves training multiple decision trees sequentially, where each subsequent tree focuses on the misclassified examples of the previous tree. This reduces the bias and improves the accuracy of the model.\n",
    "\n",
    "Deecision trees are powerful tools for classification problems that provide a clear and interpretable representation of the decision rules learned from the data. The choice of splitting criterion, stopping criterion, and ensemble method can have a significant impact on the performance and generalization of the model.\n",
    "\n",
    "\n",
    "### **Bagging**\n",
    "\n",
    "\n",
    "\n",
    "Bagging is an ensemble learning technique that aims to decrease the variance of a single estimator by combining the predictions from multiple learners. The basic idea behind bagging is to generate multiple versions of the training dataset through random sampling with replacement, and then train a separate classifier for each sampled dataset. The predictions from these individual classifiers are then combined using averaging or voting to obtain a final prediction.\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "Suppose we have a training set D of size n, and we want to train a classifier using bagging. Here are the steps involved:\n",
    "\n",
    "* Create k different bootstrap samples from D, each of size n.\n",
    "* Train a classifier on each bootstrap sample.\n",
    "* When making predictions on a new data point, take the average or majority vote of the predictions from each of the k classifiers.\n",
    "\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "\n",
    "Suppose we have a binary classification problem with classes -1 and 1. Let's also assume that we have a training set D of size n, and we want to train a decision tree classifier using bagging.\n",
    "\n",
    "**Bootstrap Sample**: For each of the k classifiers, we create a bootstrap sample of size n by sampling with replacement from D. This means that each bootstrap sample may contain duplicates of some instances and may also miss some instances from the original dataset. Let's denote the i-th bootstrap sample as D_i.\n",
    "\n",
    "**Train a Classifier**: We train a decision tree classifier T_i on each bootstrap sample D_i. This gives us k classifiers T_1, T_2, ..., T_k.\n",
    "\n",
    "**Combine Predictions**: To make a prediction on a new data point x, we take the majority vote of the predictions from each of the k classifiers.\n",
    "\n",
    "The idea behind bagging is that the variance of the prediction error decreases as k increases. This is because each classifier has a chance to explore a different part of the feature space due to the random sampling with replacement, and the final prediction is a combination of these diverse classifiers.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83600b1c",
   "metadata": {},
   "source": [
    "### **Random Forest**\n",
    "\n",
    "\n",
    "\n",
    "Random Forest is an ensemble learning algorithm that builds a large number of decision trees and combines them to make a final prediction. It is a type of bagging method, where multiple decision trees are trained on random subsets of the training data and features. The algorithm then averages the predictions of these individual trees to produce a final prediction. Random Forest is particularly useful for handling high-dimensional data and for avoiding overfitting.\n",
    "\n",
    "**Algorithm of Random Forest**\n",
    "\n",
    "The algorithm of Random Forest can be summarized in the following steps:\n",
    "\n",
    "* Start by randomly selecting a subset of the training data, with replacement. This subset is called the bootstrap sample.\n",
    "\n",
    "* Next, randomly select a subset of features from the full feature set.\n",
    "\n",
    "* Build a decision tree using the bootstrap sample and the selected subset of features. At each node of the tree, select the best feature and split the data based on the selected feature.\n",
    "\n",
    "* Repeat steps 1-3 to build multiple trees.\n",
    "\n",
    "* Finally, combine the predictions of all trees to make a final prediction. For classification, this is usually done by taking a majority vote of the predicted classes. For regression, this is usually done by taking the average of the predicted values.\n",
    "\n",
    "\n",
    "**Mathematics Behind Random Forest**\n",
    "\n",
    "The mathematics behind Random Forest involves the use of decision trees and the bootstrap sampling technique. Decision trees are constructed using a recursive binary partitioning algorithm that splits the data based on the values of the selected features. At each node, the algorithm chooses the feature and the split point that maximizes the information gain. Information gain measures the reduction in entropy or impurity of the target variable after the split. The goal is to minimize the impurity of the subsets after each split.\n",
    "\n",
    "Bootstrap sampling is a statistical technique that involves randomly sampling the data with replacement to create multiple subsets. These subsets are used to train individual decision trees. By using bootstrap samples, the algorithm can generate multiple versions of the same dataset with slightly different distributions. This introduces randomness into the training process, which helps to reduce overfitting.\n",
    "\n",
    "\n",
    "\n",
    "**Difference between Bagging and Random Forest**\n",
    "\n",
    "Bagging and Random Forest are both ensemble learning algorithms that involve training multiple models on random subsets of the data. The main difference between the two is the way the individual models are trained.\n",
    "\n",
    "Bagging involves training multiple models using the bootstrap sampling technique, but each model uses the same set of features. This can lead to correlated predictions, which reduces the variance but not necessarily the bias of the model.\n",
    "\n",
    "Random Forest, on the other hand, involves training multiple models using the bootstrap sampling technique, but each model uses a randomly selected subset of features. This introduces additional randomness into the model and helps to reduce the correlation between individual predictions. Random Forest can achieve better performance than Bagging, especially when dealing with high-dimensional data or noisy features. In simpler terms it uses subsets of observations as well as features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77bd536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "w = {0: weights[0], 1: weights[1]}\n",
    "random_f = RandomForestClassifier(n_estimators=20, class_weight=w, random_state=7)\n",
    "random_f.fit(X_train, y_train)\n",
    "\n",
    "randomf_results = evaluate_models(\"Random Forest\", random_f,X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "add_dic_to_final_df(randomf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9b9f802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_random_forest(X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test, y_test=y_test):\n",
    "  features = X_train.columns\n",
    "  labels=[\"delay\"]\n",
    "\n",
    "  with wandb.init(project=PROJECT_NAME) as run:\n",
    "      config = wandb.config\n",
    "\n",
    "      model = RandomForestClassifier(n_estimators=20, class_weight=w, random_state=7)\n",
    "\n",
    "      model.fit(X_train, y_train)\n",
    "      # train predictions\n",
    "      y_train_pred = model.predict(X_train)\n",
    "      # train performance\n",
    "      train_f1_score = f1_score(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "      # validation predictions\n",
    "      y_valid_pred = model.predict(X_valid)\n",
    "      # validation performance\n",
    "      valid_f1_score = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "\n",
    "      # test predictions\n",
    "      y_preds = model.predict(X_test)\n",
    "      y_probas = model.predict_proba(X_test)\n",
    "\n",
    "      score = f1_score(y_test, y_preds)\n",
    "      print(f\"F1_score Train: {round(train_f1_score, 4)}\")\n",
    "      print(f\"F1_score Valid: {round(valid_f1_score, 4)}\")\n",
    "      print(f\"F1_score Test: {round(score, 4)}\")\n",
    "\n",
    "\n",
    "      wandb.log({\"f1_score_train\": train_f1_score})\n",
    "      wandb.log({\"f1_score_valid\": valid_f1_score})\n",
    "      wandb.log({\"f1_score\": score})\n",
    "\n",
    "\n",
    "\n",
    "      wandb.sklearn.plot_classifier(model, X_train, X_test, y_train, y_test, y_preds, y_probas, labels=None,\n",
    "                                                          model_name='RandomForestClassifier', feature_names=features)\n",
    "\n",
    "      model_artifact = wandb.Artifact(\n",
    "                  \"RandomForestClassifier\", type=\"model\",metadata=dict(config))\n",
    "\n",
    "      joblib.dump(model, \"randomf-truck-model.pkl\")\n",
    "      model_artifact.add_file(\"randomf-truck-model.pkl\")\n",
    "      wandb.save(\"randomf-truck-model.pkl\")\n",
    "      run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_forest(X_train, y_train,X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gradient Boosting**\n",
    "\n",
    "The primary idea behind this technique is to develop models in a sequential manner, with each model attempting to reduce the mistakes of the previous model.The additive model, loss function, and a weak learner are the three fundamental components of Gradient Boosting.\n",
    "\n",
    "The method provides a direct interpretation of boosting in terms of numerical optimization of the loss function using Gradient Descent. We employ Gradient Boosting Regressor when the target column is continuous, and Gradient Boosting Classifier when the task is a classification problem. The \"Loss function\" is the only difference between the two. The goal is to use gradient descent to reduce this loss function by adding weak learners. Because it is based on loss functions, for regression problems, Mean squared error (MSE) will be used, and  for classification problems, log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8a69b",
   "metadata": {},
   "source": [
    "### **XG Boost**\n",
    "\n",
    "\n",
    "XGBoost is a variant of gradient boosting, which is a popular ensemble learning technique that works by iteratively adding new models to an ensemble, each model attempting to correct the errors made by the previous models. In each iteration, the algorithm calculates the negative gradient of the loss function with respect to the current prediction, and fits a new model to the residual errors. The new model is then added to the ensemble, and the algorithm repeats this process until the desired number of models is reached.\n",
    "\n",
    "In XGBoost, the objective function is used to measure the difference between the predicted values and the true labels. The objective function is a sum of the loss function and the regularization term, where the latter prevents overfitting and encourages the model to be simple.\n",
    "\n",
    "\n",
    "\n",
    "Suppose we have a dataset with three features, x1, x2, and x3, and we want to predict a binary outcome, y. We decide to use decision trees as our weak learners. We start by training a decision tree on the entire dataset. However, this decision tree may not be able to capture the complex relationships between the features and the outcome, and it may be overfitting the training data.\n",
    "\n",
    "To improve upon the first decision tree, we can use XGBoost. Here's how:\n",
    "\n",
    "* Initialize the model: We start by initializing the XGBoost model with default hyperparameters. This model will be a simple decision tree with a single split.\n",
    "\n",
    "* Make predictions: We use this model to make predictions on the training data. We compare these predictions to the true labels and calculate the residuals, which are the differences between the predicted values and the true labels.\n",
    "\n",
    "* Fit a new tree: We then fit a new decision tree to the residuals. This tree will be a weak learner, as it is only modeling the errors of the previous model.\n",
    "\n",
    "* Combine the models: We add the new tree to the previous model to create a new ensemble. This new ensemble consists of the previous model plus the new tree.\n",
    "\n",
    "* Repeat: We repeat steps 2-4 for a specified number of iterations, adding a new tree to the ensemble each time.\n",
    "\n",
    "* Predictions: To make predictions on new data, we combine the predictions of all the trees in the ensemble.\n",
    "\n",
    "The key idea behind XGBoost is that it improves upon the predictions of the weak learners by focusing on the misclassified data points. By fitting a new tree to the residuals, XGBoost can correct the errors of the previous model and improve its overall accuracy. Additionally, XGBoost uses regularization to prevent overfitting and to improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **XG Boost Training**\n",
    "\n",
    "XGBoost is a popular gradient boosting library used for building supervised machine learning models. It is designed to be efficient, scalable, and flexible. XGBoost provides an efficient implementation of gradient boosting algorithms and is widely used in various machine learning competitions.\n",
    "\n",
    "In XGBoost, the D matrix is a data structure that stores the input data and provides efficient access to it during model training. The D matrix is essentially a wrapper around the input data, which is typically stored as a two-dimensional NumPy array or a Pandas DataFrame. The D matrix adds some additional features to the input data that make it easier to use with XGBoost.\n",
    "\n",
    "The D matrix provides a few benefits over using the raw input data directly. First, it allows for efficient access to the input data during model training, which is critical for large datasets. Second, it provides some additional functionality, such as the ability to handle missing values and to split the data into training and validation sets. Finally, it simplifies the process of passing data to the XGBoost model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ed9c421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mlogloss:0.61153\n",
      "[1]\tvalidation-mlogloss:0.56986\n",
      "[2]\tvalidation-mlogloss:0.54929\n",
      "[3]\tvalidation-mlogloss:0.53773\n",
      "[4]\tvalidation-mlogloss:0.53350\n",
      "[5]\tvalidation-mlogloss:0.52825\n",
      "[6]\tvalidation-mlogloss:0.52923\n",
      "[7]\tvalidation-mlogloss:0.52953\n",
      "[8]\tvalidation-mlogloss:0.52742\n",
      "[9]\tvalidation-mlogloss:0.52832\n",
      "[10]\tvalidation-mlogloss:0.52875\n",
      "[11]\tvalidation-mlogloss:0.52921\n",
      "[12]\tvalidation-mlogloss:0.53262\n",
      "[13]\tvalidation-mlogloss:0.53315\n",
      "[14]\tvalidation-mlogloss:0.53715\n",
      "[15]\tvalidation-mlogloss:0.53819\n",
      "[16]\tvalidation-mlogloss:0.53824\n",
      "[17]\tvalidation-mlogloss:0.53847\n",
      "[18]\tvalidation-mlogloss:0.53955\n",
      "Train Results\n",
      "F1 Score: 0.7202122527737578\n",
      "Recall Score: 0.5912871287128713\n",
      "Confusion Matrix: \n",
      "[[5145  128]\n",
      " [1032 1493]]\n",
      "Area Under Curve: 0.7835062611135\n",
      " \n",
      "Validation Results\n",
      "F1 Score: 0.593440122044241\n",
      "Recall Score: 0.4832298136645963\n",
      "Confusion Matrix: \n",
      "[[1302  117]\n",
      " [ 416  389]]\n",
      "Area Under Curve: 0.7003886911874778\n",
      " \n",
      "Test Results\n",
      "F1 Score: 0.6998468606431854\n",
      "Recall Score: 0.5973856209150327\n",
      "Confusion Matrix: \n",
      "[[829  84]\n",
      " [308 457]]\n",
      "Area Under Curve: 0.7526906198770126\n"
     ]
    }
   ],
   "source": [
    "# import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Convert training and test sets to DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Train initial model\n",
    "params = {'objective': 'multi:softmax', 'num_class': 2, 'seed': 7}\n",
    "num_rounds = 30\n",
    "xgbmodel = xgb.train(params, dtrain, num_rounds, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
    "\n",
    "xgb_results = evaluate_models(\"XGB\", xgbmodel, dtrain, y_train, dvalid, y_valid, dtest, y_test)\n",
    "add_dic_to_final_df(xgb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63d34adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_xgb_model(X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test, y_test=y_test):\n",
    "  features = X_train.columns\n",
    "  labels=[\"delay\"]\n",
    "\n",
    "  with wandb.init(project=PROJECT_NAME) as run:\n",
    "      config = wandb.config\n",
    "\n",
    "\n",
    "      # Convert training and test sets to DMatrix\n",
    "      dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "      dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "      dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "      # Train initial model\n",
    "      params = {'objective': 'multi:softmax', 'num_class': 2}\n",
    "      num_rounds = 30\n",
    "      xgbmodel = xgb.train(params, dtrain, num_rounds, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
    "        \n",
    "      # train predictions\n",
    "      y_train_pred = xgbmodel.predict(dtrain)\n",
    "      # train performance\n",
    "      train_f1_score = f1_score(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "      # validation predictions\n",
    "      y_valid_pred = xgbmodel.predict(dvalid)\n",
    "      # validation performance\n",
    "      valid_f1_score = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "\n",
    "      # test predictions\n",
    "      y_preds = xgbmodel.predict(dtest)\n",
    "      score = f1_score(y_test, y_preds)\n",
    "      print(f\"F1_score Train: {round(train_f1_score, 4)}\")\n",
    "      print(f\"F1_score Valid: {round(valid_f1_score, 4)}\")\n",
    "      print(f\"F1_score Test: {round(score, 4)}\")\n",
    "\n",
    "\n",
    "      wandb.log({\"f1_score_train\": train_f1_score})\n",
    "      wandb.log({\"f1_score_valid\": valid_f1_score})\n",
    "      wandb.log({\"f1_score\": score})\n",
    "\n",
    "\n",
    "      model_artifact = wandb.Artifact(\n",
    "                  \"XGBoost\", type=\"model\",metadata=dict(config))\n",
    "\n",
    "      joblib.dump(xgbmodel, \"xgb-truck-model.pkl\")\n",
    "      model_artifact.add_file(\"xgb-truck-model.pkl\")\n",
    "      wandb.save(\"xgb-truck-model.pkl\")\n",
    "      run.log_artifact(model_artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e1eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_model(X_train, y_train,X_valid, y_valid, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c83703",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee2b09d",
   "metadata": {},
   "source": [
    "### **Model Building Summary**\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "* The F1 score and recall on the training set are relatively low, indicating that the model might not fit the training data well.\n",
    "* The F1 score and recall on the validation and test sets are high but we'll have to look for consistency of  the model results over high scores.\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "* The Random Forest model has very high F1 scores and recall on the training set, indicating a potential overfitting issue.\n",
    "* On the validation set, the F1 score is lower than on the training set, which is expected, but it's still relatively high.\n",
    "* The model performs well on the test set with a better F1 score and recall.\n",
    "* To address potential overfitting, we can experiment with reducing the complexity of the Random Forest model, by limiting the depth of the trees.\n",
    "\n",
    "XGBoost:\n",
    "\n",
    "* XGBoost shows good F1 scores and recall on the training set, indicating a reasonable fit to the training data.\n",
    "* On the validation set, the F1 score is reasonable, suggesting decent generalization.\n",
    "* The model performs well on the test set with a high F1 score and recall, indicating good generalization to unseen data.\n",
    "* XGBoost seems to be the most promising model, we can do hyperparameter tuning to see if we can improve its performance even more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e227ed",
   "metadata": {},
   "source": [
    "## **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8964cd3",
   "metadata": {},
   "source": [
    "### Hyperparameter Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e88fa5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "w = {0: weights[0], 1: weights[1]}\n",
    "def train_rf_model(X_train=X_train, y_train=y_train, X_valid=X_valid,y_valid=y_valid, X_test=X_test, y_test=y_test):\n",
    "    features = X_train.columns\n",
    "\n",
    "    with wandb.init(\n",
    "        project=PROJECT_NAME ) as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=config[\"n_estimators\"],\n",
    "            max_depth=config[\"max_depth\"],\n",
    "            min_samples_split=config[\"min_samples_split\"],\n",
    "            random_state=7,\n",
    "            class_weight=w\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # train predictions\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        # train performance\n",
    "        train_f1_score = f1_score(y_train, y_train_pred)\n",
    "        \n",
    "\n",
    "        # validation predictions\n",
    "        y_valid_pred = model.predict(X_valid)\n",
    "        # validation performance\n",
    "        valid_f1_score = f1_score(y_valid, y_valid_pred)\n",
    "      \n",
    "\n",
    "        y_preds = model.predict(X_test)\n",
    "        y_probas = model.predict_proba(X_test)\n",
    "\n",
    "        score = f1_score(y_test, y_preds)\n",
    "        print(f\"F1_score Train: {round(train_f1_score, 4)}\")\n",
    "        print(f\"F1_score Valid: {round(valid_f1_score, 4)}\")\n",
    "        print(f\"F1_score Test: {round(score, 4)}\")\n",
    "\n",
    "\n",
    "        wandb.log({\"f1_score_train\": train_f1_score})\n",
    "        wandb.log({\"f1_score_valid\": valid_f1_score})\n",
    "        wandb.log({\"f1_score\": score})\n",
    "\n",
    "        wandb.sklearn.plot_classifier(model, X_train, X_test, y_train, y_test, y_preds, y_probas, labels=None,\n",
    "                                                          model_name='RandomForestClassifier', feature_names=features)\n",
    "\n",
    "        model_artifact = wandb.Artifact(\n",
    "            \"RandomForestClassifier\", type=\"model\",metadata=dict(config))\n",
    "        joblib.dump(model, \"random_f_tuned.pkl\")\n",
    "        model_artifact.add_file(\"random_f_tuned.pkl\")\n",
    "        wandb.save(\"random_f_tuned.pkl\")\n",
    "        run.log_artifact(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e821a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_f.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec255443",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configs = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"f1_score\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"n_estimators\": {\n",
    "            \"values\": [8, 12, 16,20]\n",
    "        },\n",
    "        \"max_depth\": {\n",
    "            \"values\": [None, 5, 10, 15, 20]\n",
    "        },\n",
    "        \"min_samples_split\": {\n",
    "            \"values\": [2, 4, 8, 12]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# Then we initialize the sweep and run the sweep agent.\n",
    "\n",
    "sweep_id = wandb.sweep(\n",
    "    sweep=sweep_configs,\n",
    "    project=PROJECT_NAME\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "wandb.agent(\n",
    "    project=PROJECT_NAME,\n",
    "    sweep_id=sweep_id,\n",
    "    function=train_rf_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Try it out-I: Hyperparameter tuning with XGBoost**\n",
    "\n",
    "Perform hyperparameter tuning on the XGBoost model using a grid search approach. Use the GridSearchCV function from the sklearn.model_selection module to search over a range of hyperparameters or Weights and Biases.\n",
    "\n",
    "Some hyperparameters you may want to consider tuning include:\n",
    "\n",
    "* max_depth: the maximum depth of each tree in the ensemble\n",
    "* learning_rate: the learning rate for gradient boosting\n",
    "* n_estimators: the number of trees in the ensemble\n",
    "* min_child_weight: the minimum weight required in a child node to continue splitting\n",
    "* subsample: the fraction of samples used for each tree\n",
    "* colsample_bytree: the fraction of features used for each tree\n",
    "* You can define a dictionary of hyperparameter values to search over, and then pass it to the param_grid parameter of the GridSearchCV function.\n",
    "\n",
    "Evaluate the tuned model\n",
    "Evaluate the performance of the tuned model on the testing data using the sklearn.metrics module.\n",
    "\n",
    "**Bonus (optional)**\n",
    "Try using a different hyperparameter optimization technique, such as random search or Bayesian optimization, to see if you can improve the performance of the XGBoost model even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Try It Out-II: Explore Feature Selection for Interpretability**\n",
    "\n",
    "Dive into feature selection techniques to improve the interpretability and explainability of your machine learning model. Start by identifying all relevant features in your dataset and categorize them based on their importance. Research and implement feature importance techniques such as permutation importance or SHAP values. Visualize the results and understand how each feature contributes to the model's predictions. Engage with business stakeholders to discuss the importance of interpretability and present your findings, highlighting the impact of each feature on decision-making. Evaluate the trade-offs between model accuracy and interpretability, and gather feedback on stakeholders' comfort levels. The goal is to ensure that the chosen features align with business priorities and empower stakeholders to make informed decisions based on the model's insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "\n",
    "\n",
    "In this part, we went deeper into solving the challenge faced by the logistics industry—delayed truck shipments. We focused on improving our ability to predict these delays accurately. Here's a simple breakdown of what we did:\n",
    "\n",
    "\n",
    "* Used the Hopsworks feature store to efficiently retrieve data.\n",
    "\n",
    "* Split data into training, validation, and test sets, ensuring our models learn and perform well on new data. Handled missing values to keep our predictions accurate.\n",
    "\n",
    "* Built and experimented with logistic regression, random forest, and XGBoost models to find the best performers.\n",
    "\n",
    "* Used hyperparameter tuning techniques to fine-tune our models for better accuracy.\n",
    "\n",
    "* Created a Streamlit application for easy interaction with our models, making insights accessible.\n",
    "\n",
    "As we wrap up this part, we've covered a lot! The next step, in Part 3, will be automating our processes, building CI/CD pipeline and making our models ready for real-world use. We hope you're as excited as we are for the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interview Questions**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* How did you handle null values in data during data preprocessing?\n",
    "* Explain the concept of train-validation-test split and its importance.\n",
    "* Why is feature scaling necessary in machine learning, and what are the different techniques you used?\n",
    "\n",
    "* Walk through the process of building logistic regression, random forest, and XGBoost models.\n",
    "* What metrics would you consider for evaluating model performance, and why?\n",
    "* Explain the importance of experiment tracking in machine learning projects.\n",
    "* How did you set up a new project and connect it to Python using Weights and Biases?\n",
    "\n",
    "* What is the purpose of hyperparameter tuning, and how does it improve model performance?\n",
    "* Describe the difference between grid search and random search for hyperparameter tuning.\n",
    "\n",
    "* How did you develop the Streamlit application, and what role does it play in this project?\n",
    "* Discuss the steps involved in deploying the Streamlit application on AWS EC2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
